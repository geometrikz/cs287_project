{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::448807757624:role/service-role/AmazonSageMaker-ExecutionRole-20211202T101582\n",
      "sagemaker bucket: sagemaker-us-east-2-448807757624\n",
      "sagemaker session region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker.huggingface\n",
    "import sagemaker\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix = 'datasets/xsum_corrupted'\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 1,\n",
    "                 'model_name':'facebook/bart-large',\n",
    "                 'output_dir':'/opt/ml/checkpoints'\n",
    "                 }\n",
    "\n",
    "# s3 uri where our checkpoints will be uploaded during training\n",
    "job_name = \"bart-large-spot-p3-16xlarge\"\n",
    "checkpoint_s3_uri = f's3://{sess.default_bucket()}/{job_name}/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='train_xsum.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.p3.16xlarge',\n",
    "                            instance_count=1,\n",
    "                            base_job_name=job_name,\n",
    "                            checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "#                             use_spot_instances=True,\n",
    "#                             max_wait=7200, # This should be equal to or greater than max_run in seconds'\n",
    "                            max_run=50000, # expected max run in seconds\n",
    "                            role=role,\n",
    "                            transformers_version='4.6',\n",
    "                            pytorch_version='1.7',\n",
    "                            py_version='py36',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-06 15:40:23 Starting - Starting the training job...\n",
      "2021-12-06 15:40:25 Starting - Launching requested ML instancesProfilerReport-1638805222: InProgress\n",
      ".........\n",
      "2021-12-06 15:42:06 Starting - Preparing the instances for training......\n",
      "2021-12-06 15:43:24 Downloading - Downloading input data......\n",
      "2021-12-06 15:44:07 Training - Downloading the training image............\n",
      "2021-12-06 15:46:15 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:16,022 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:16,101 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:17,520 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:18,084 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 1,\n",
      "        \"model_name\": \"facebook/bart-large\",\n",
      "        \"output_dir\": \"/opt/ml/checkpoints\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"bart-large-spot-p3-16xlarge-2021-12-06-15-40-22-513\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-448807757624/bart-large-spot-p3-16xlarge-2021-12-06-15-40-22-513/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_xsum\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_xsum.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"facebook/bart-large\",\"output_dir\":\"/opt/ml/checkpoints\",\"train_batch_size\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_xsum.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_xsum\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-448807757624/bart-large-spot-p3-16xlarge-2021-12-06-15-40-22-513/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"facebook/bart-large\",\"output_dir\":\"/opt/ml/checkpoints\",\"train_batch_size\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"bart-large-spot-p3-16xlarge-2021-12-06-15-40-22-513\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-448807757624/bart-large-spot-p3-16xlarge-2021-12-06-15-40-22-513/source/sourcedir.tar.gz\",\"module_name\":\"train_xsum\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_xsum.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"facebook/bart-large\",\"--output_dir\",\"/opt/ml/checkpoints\",\"--train_batch_size\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=facebook/bart-large\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train_xsum.py --epochs 1 --model_name facebook/bart-large --output_dir /opt/ml/checkpoints --train_batch_size 1\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:22,508 - __main__ - INFO - ['train_xsum.py', '--epochs', '1', '--model_name', 'facebook/bart-large', '--output_dir', '/opt/ml/checkpoints', '--train_batch_size', '1']\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:22,596 - __main__ - INFO -  loaded train_dataset length is: 135750\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:22,674 - filelock - INFO - Lock 139851765391880 acquired on /root/.cache/huggingface/transformers/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.58d5dda9f4e9f44e980adb867b66d9e0cbe3e0c05360cefe3cd86f5db4fff042.lock\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:22,750 - filelock - INFO - Lock 139851765391880 released on /root/.cache/huggingface/transformers/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.58d5dda9f4e9f44e980adb867b66d9e0cbe3e0c05360cefe3cd86f5db4fff042.lock\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:22,894 - filelock - INFO - Lock 139851108587504 acquired on /root/.cache/huggingface/transformers/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05.lock\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:23,058 - filelock - INFO - Lock 139851108587504 released on /root/.cache/huggingface/transformers/0d6fc8b2ef1860c1f8f0baff4b021e3426cc7d11b153f98e563b799603ee2f25.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05.lock\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:23,137 - filelock - INFO - Lock 139851108587504 acquired on /root/.cache/huggingface/transformers/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:23,283 - filelock - INFO - Lock 139851108587504 released on /root/.cache/huggingface/transformers/6e75e35f0bdd15870c98387e13b93a8e100237eb33ad99c36277a0562bd6d850.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:23,361 - filelock - INFO - Lock 139851108587504 acquired on /root/.cache/huggingface/transformers/d94f53c8851dcda40774f97280e634b94b721a58e71bcc152b5f51d0d49a046a.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:23,529 - filelock - INFO - Lock 139851108587504 released on /root/.cache/huggingface/transformers/d94f53c8851dcda40774f97280e634b94b721a58e71bcc152b5f51d0d49a046a.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:23,749 - filelock - INFO - Lock 139851108587504 acquired on /root/.cache/huggingface/transformers/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8.lock\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:23,818 - filelock - INFO - Lock 139851108587504 released on /root/.cache/huggingface/transformers/1abf196c889c24daca2909359ca2090e5fcbfa21a9ea36d763f70adbafb500d7.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8.lock\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:24,072 - filelock - INFO - Lock 139851108689568 acquired on /root/.cache/huggingface/transformers/d065edfe6954baf0b989a2063b26eb07e8c4d0b19354b5c74af9a51f5518df6e.6ca4df1a6ec59aa763989ceec10dff41dde19f0f0824b9f5d3fcd35a8abffdb2.lock\u001b[0m\n",
      "\u001b[34m2021-12-06 15:46:42,072 - filelock - INFO - Lock 139851108689568 released on /root/.cache/huggingface/transformers/d065edfe6954baf0b989a2063b26eb07e8c4d0b19354b5c74af9a51f5518df6e.6ca4df1a6ec59aa763989ceec10dff41dde19f0f0824b9f5d3fcd35a8abffdb2.lock\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.555 algo-1:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.692 algo-1:26 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.693 algo-1:26 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.694 algo-1:26 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.696 algo-1:26 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.879 algo-1:26 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.879 algo-1:26 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.879 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.879 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.880 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.881 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.882 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.883 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.884 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.885 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.886 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.887 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.888 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.889 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.890 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.891 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.892 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.893 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.893 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.893 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.893 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.893 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.893 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.893 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.893 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.893 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.893 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.893 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.893 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.894 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.895 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.895 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.895 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.895 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.895 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.895 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.895 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.895 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.895 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.895 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.895 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.896 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.896 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.896 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.896 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.896 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.896 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.896 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.896 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.896 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.896 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.896 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.896 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.896 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.897 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.898 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.899 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.900 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.901 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.902 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.902 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.902 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.902 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.902 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.902 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.902 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.902 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.902 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.902 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.902 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.902 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.902 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.903 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.904 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.905 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.906 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.907 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.908 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.909 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.910 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.911 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.912 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.913 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.913 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.913 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.913 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.913 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.913 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.913 algo-1:26 INFO hook.py:591] name:module.model.decoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.913 algo-1:26 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.913 algo-1:26 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.913 algo-1:26 INFO hook.py:593] Total Trainable Params: 406291456\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.913 algo-1:26 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-12-06 15:46:52.916 algo-1:26 INFO hook.py:488] Hook is writing from the hook with pid: 26\u001b[0m\n",
      "\u001b[34mNCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m{'loss': 0.4539, 'learning_rate': 2.8127505010020042e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m{'loss': 0.3169, 'learning_rate': 2.438251503006012e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2505, 'learning_rate': 2.06375250501002e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2158, 'learning_rate': 1.689253507014028e-05, 'epoch': 0.47}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({'train': training_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
