{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers sentence-transformers stanza datasets rouge_score spacy\n",
    "# ! python -m spacy download en_core_web_lg\n",
    "\n",
    "import stanza\n",
    "\n",
    "corenlp_dir = './corenlp'\n",
    "stanza.install_corenlp(dir=corenlp_dir)\n",
    "\n",
    "# Set the CORENLP_HOME environment variable to point to the installation location\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
    "\n",
    "!ls $CORENLP_HOME\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from stanza.server import CoreNLPClient\n",
    "!pip install googletrans==4.0.0-rc1\n",
    "import pandas as pd\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "\n",
    "import random\n",
    "import spacy\n",
    "spacy = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_ws(old_token, new_token):\n",
    "    # Align trailing whitespaces between tokens\n",
    "    if old_token[-1] == new_token[-1] == \" \":\n",
    "        return new_token\n",
    "    elif old_token[-1] == \" \":\n",
    "        return new_token + \" \"\n",
    "    elif new_token[-1] == \" \":\n",
    "        return new_token[:-1]\n",
    "    else:\n",
    "        return new_token\n",
    "    \n",
    "NEGATABLE_TOKENS = (\"are\", \"is\", \"was\", \"were\", \"have\", \"has\", \"had\",\n",
    "                                   \"do\", \"does\", \"did\", \"can\", \"ca\", \"could\", \"may\",\n",
    "                                   \"might\", \"must\", \"shall\", \"should\", \"will\", \"would\")\n",
    "\n",
    "\n",
    "\n",
    "def negation(summary):\n",
    "  candidate_tokens = [token for token in summary if token.text in NEGATABLE_TOKENS]\n",
    "\n",
    "  if not candidate_tokens:\n",
    "      return None, None\n",
    "\n",
    "  # choose random token to negate\n",
    "  negated_token = random.choice(candidate_tokens)\n",
    "  negated_index = negated_token.i\n",
    "  L = len(summary)\n",
    "\n",
    "  # negation occurs at the first negatable token (e.g. does not have)\n",
    "  if negated_index > 0:\n",
    "    if summary[negated_index-1].text in NEGATABLE_TOKENS:\n",
    "      negated_token = summary[negated_index-1]\n",
    "      negated_index = negated_index-1\n",
    "  \n",
    "  #check whether qualified by a negative\n",
    "  is_negative = False\n",
    "  if (L-1) > negated_index:\n",
    "    if summary[negated_index + 1].text in [\"not\", \"n't\"]:\n",
    "      is_negative = True\n",
    "    elif summary[negated_index+1].text == \"no\":\n",
    "      return None, None\n",
    "\n",
    "  #add not/n't if is_negative is False, remove if True\n",
    "  tokens = [token.text_with_ws for token in summary]\n",
    "  if is_negative:\n",
    "      if summary[negated_index + 1].text.lower() == \"n't\":\n",
    "          if summary[negated_index + 1].text.lower() == \"ca\":\n",
    "              tokens[negated_index] = \"can\" if tokens[negated_index].islower() else \"Can\"\n",
    "          tokens[negated_index] = tokens[negated_index] + \" \"\n",
    "      tokens.pop(negated_index + 1)\n",
    "\n",
    "  else:\n",
    "      if summary[negated_index].text.lower() in [\"am\", \"may\", \"might\", \"must\", \"shall\", \"will\"]:\n",
    "          negation = \"not \"\n",
    "      else:\n",
    "          negation = random.choice([\"not \", \"n't \"])\n",
    "\n",
    "      if negation == \"n't \":\n",
    "          if summary[negated_index].text.lower() == \"can\":\n",
    "              tokens[negated_index] = \"ca\" if tokens[negated_index].islower() else \"Ca\"\n",
    "          else:\n",
    "              tokens[negated_index] = tokens[negated_index][:-1]\n",
    "      tokens.insert(negated_index + 1, negation)\n",
    "  \n",
    "  new_summary = spacy(\"\".join(tokens))\n",
    "  augmentation_span = [(negated_index, negated_index if is_negative else negated_index + 1)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "CLASS_TO_PRONOUN = {\n",
    "            \"SUBJECT\": [\"you\", \"he\", \"she\", \"we\", \"they\"],\n",
    "            \"OBJECT\": [\"me\", \"you\", \"him\", \"her\", \"us\", \"them\"],\n",
    "            \"POSSESSIVE\": [\"my\", \"your\", \"his\", \"her\", \"its\", \"our\", \"your\", \"their\"],\n",
    "            \"REFLEXIVE\": [\"myself\", \"yourself\", \"himself\", \"itself\", \"ourselves\", \"yourselves\", \"themselves\"]\n",
    "        }\n",
    "\n",
    "\n",
    "PRONOUN_TO_CLASS = {pronoun: key for (key, values) in CLASS_TO_PRONOUN.items() for pronoun in values}\n",
    "PRONOUNS = [pronoun for pronoun in PRONOUN_TO_CLASS.keys()]\n",
    "\n",
    "\n",
    "def pronounswap(summary):\n",
    "  summary_pronouns = [token for token in summary if token.text.lower() in PRONOUNS]\n",
    "\n",
    "  if not summary_pronouns:\n",
    "    return None, None\n",
    "\n",
    "  swap = random.choice(summary_pronouns)\n",
    "  swap_index = swap.i\n",
    "  swap_class = PRONOUN_TO_CLASS[swap.text.lower()]\n",
    "\n",
    "  candidate_tokens = [token for token in CLASS_TO_PRONOUN[swap_class] if token != swap.text.lower()]\n",
    "\n",
    "  if not candidate_tokens:\n",
    "    return None, None\n",
    "\n",
    "  swapped_token = random.choice(candidate_tokens)\n",
    "  swapped_token = align_ws(swap.text_with_ws, swapped_token)\n",
    "  swapped_token = swapped_token if swap.text.islower() else swapped_token.capitalize()\n",
    "\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  summary_tokens[swap_index] = swapped_token\n",
    "\n",
    "  new_summary = spacy(\"\".join(summary_tokens))\n",
    "\n",
    "  augmentation_span = [(swap_index, swap_index)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "\n",
    "ENTITY_CATEGORIES = (\"PERSON\", \"ORG\", \"NORP\", \"FAC\", \"GPE\", \"LOC\", \"PRODUCT\",\n",
    "                           \"WORK_OF_ART\", \"EVENT\")\n",
    "\n",
    "def entityswap(summary, source):\n",
    "  source_ents = [ent for ent in source.ents if ent.label_ in ENTITY_CATEGORIES]\n",
    "  summary_ents = [ent for ent in summary.ents if ent.label_ in ENTITY_CATEGORIES]\n",
    "\n",
    "  if not source_ents or not summary_ents:\n",
    "    return None, None\n",
    "\n",
    "  swap = random.choice(summary_ents)\n",
    "  candidate_ents = [ent for ent in source_ents if ent.text != swap.text and ent.text not in swap.text and swap.text not in ent.text]\n",
    "\n",
    "  if not candidate_ents:\n",
    "    return None, None\n",
    "\n",
    "  swapped_ent = random.choice(candidate_ents)\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  swapped_token = align_ws(swap.text_with_ws, swapped_ent.text_with_ws)\n",
    "  summary_tokens = summary_tokens[:swap.start] + [swapped_token] + summary_tokens[swap.end:]\n",
    "\n",
    "  new_summary = spacy(\"\".join(summary_tokens))\n",
    "  augmentation_span = [(swap.start, swap.start + len(swapped_ent) - 1)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "NUMBER_CATEGORIES = (\"PERCENT\", \"MONEY\", \"QUANTITY\", \"CARDINAL\")\n",
    "\n",
    "def numberswap(summary, source):\n",
    "  source_ents = [ent for ent in source.ents if ent.label_ in NUMBER_CATEGORIES]\n",
    "  summary_ents = [ent for ent in summary.ents if ent.label_ in NUMBER_CATEGORIES]\n",
    "\n",
    "  if not source_ents or not summary_ents:\n",
    "    return None, None\n",
    "\n",
    "  swap = random.choice(summary_ents)\n",
    "  candidate_ents = [ent for ent in source_ents if ent.text != swap.text and ent.text not in swap.text and swap.text not in ent.text]\n",
    "\n",
    "  if not candidate_ents:\n",
    "    return None, None\n",
    "\n",
    "  swapped_ent = random.choice(candidate_ents)\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  swapped_token = align_ws(swap.text_with_ws, swapped_ent.text_with_ws)\n",
    "  summary_tokens = summary_tokens[:swap.start] + [swapped_token] + summary_tokens[swap.end:]\n",
    "\n",
    "  new_summary = spacy(\"\".join(summary_tokens))\n",
    "  augmentation_span = [(swap.start, swap.start + len(swapped_ent) - 1)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "DATE_CATEGORIES = (\"DATE\", \"TIME\")\n",
    "\n",
    "def dateswap(summary, source):\n",
    "  source_ents = [ent for ent in source.ents if ent.label_ in DATE_CATEGORIES]\n",
    "  summary_ents = [ent for ent in summary.ents if ent.label_ in DATE_CATEGORIES]\n",
    "\n",
    "  if not source_ents or not summary_ents:\n",
    "    return None, None\n",
    "\n",
    "  swap = random.choice(summary_ents)\n",
    "  candidate_ents = [ent for ent in source_ents if ent.text != swap.text and ent.text not in swap.text and swap.text not in ent.text]\n",
    "\n",
    "  if not candidate_ents:\n",
    "    return None, None\n",
    "\n",
    "  swapped_ent = random.choice(candidate_ents)\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  swapped_token = align_ws(swap.text_with_ws, swapped_ent.text_with_ws)\n",
    "  summary_tokens = summary_tokens[:swap.start] + [swapped_token] + summary_tokens[swap.end:]\n",
    "\n",
    "  new_summary = spacy(\"\".join(summary_tokens))\n",
    "  augmentation_span = [(swap.start, swap.start + len(swapped_ent) - 1)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "\n",
    "POS_TAGS = (\"NOUN\", \"ADJ\", \"PROPN\")\n",
    "\n",
    "\n",
    "def openIE_filter(extract, summary):\n",
    "  new_text = \"\"\n",
    "  extract = spacy(extract)\n",
    "\n",
    "  start_index = 0\n",
    "  for token in summary:\n",
    "    if token.text != extract[0].text:\n",
    "      start_index+=1\n",
    "    else:\n",
    "      break\n",
    "  tracking_index = start_index\n",
    "  for word in extract:\n",
    "    while (summary[tracking_index].text != word.text):\n",
    "      tracking_index+=1\n",
    "    if summary[tracking_index].pos_ in POS_TAGS:\n",
    "      break\n",
    "  start_index = tracking_index\n",
    "\n",
    "  while (summary[tracking_index].pos_ != \"NOUN\" and summary[tracking_index].pos_ != \"PROPN\"):\n",
    "    new_text+=(summary[tracking_index].text_with_ws)\n",
    "    tracking_index += 1\n",
    "  while (summary[tracking_index].pos_ == \"NOUN\" or summary[tracking_index].pos_ == \"PROPN\"):\n",
    "    new_text+=(summary[tracking_index].text_with_ws)\n",
    "    tracking_index += 1\n",
    "  return new_text, start_index, tracking_index\n",
    "  \n",
    "\n",
    "\n",
    "# def s_o_swap(summary):\n",
    "#   summary_ents = [ent for ent in summary.ents]\n",
    "#   client = CoreNLPClient(timeout=150000000, be_quiet=True, annotators=['openie'], \n",
    "#   endpoint='http://localhost:9002')\n",
    "#   client.start()\n",
    "#   import time\n",
    "#   time.sleep(2)\n",
    "#   story_triples = []\n",
    "\n",
    "#   document = client.annotate(summary.text, output_format='json')\n",
    "#   triples = []\n",
    "#   for sentence in document['sentences']:\n",
    "#       for triple in sentence['openie']:\n",
    "#           triples.append({\n",
    "#             'subject': triple['subject'],\n",
    "#             'relation': triple['relation'],\n",
    "#               'object': triple['object']\n",
    "#           })\n",
    "  \n",
    "#   client.stop()\n",
    "#   time.sleep(2)\n",
    "\n",
    "#   if not triples:\n",
    "#     return None, None\n",
    "#   candidate_triples = []\n",
    "#   for triple in triples:\n",
    "#     subj = triple['subject']\n",
    "#     obj = triple['object']\n",
    "#     allow_triple = False\n",
    "#     for ent in summary_ents:\n",
    "#       if subj == ent.text or subj in ent.text or ent.text in subj:\n",
    "#         allow_triple = True\n",
    "#       if obj == ent.text or obj in ent.text or ent.text in obj:\n",
    "#         allow_triple = True\n",
    "#     if allow_triple == True:\n",
    "#       candidate_triples.append(triple)\n",
    "#   if not candidate_triples:\n",
    "#     return None, None\n",
    "#   triple_swap = random.choice(candidate_triples)\n",
    "#   triple_swap[\"subject\"], s_start, s_end = openIE_filter(triple_swap[\"subject\"], summary)\n",
    "#   triple_swap[\"object\"], o_start, o_end = openIE_filter(triple_swap[\"object\"], summary)\n",
    "#   if s_start == 0:\n",
    "#     triple_swap[\"object\"] = triple_swap[\"object\"].capitalize()\n",
    "#     if summary[s_start].pos_ != \"PROPN\":\n",
    "#       triple_swap[\"subject\"] = triple_swap[\"subject\"][0].lower() + triple_swap[\"subject\"][1:]\n",
    "#   if o_start == 0:\n",
    "#     triple_swap[\"subject\"] = triple_swap[\"subject\"].capitalize()\n",
    "#     if summary[o_start].pos_ != \"PROPN\":\n",
    "#       triple_swap[\"object\"] = triple_swap[\"object\"][0].lower() + triple_swap[\"object\"][1:]\n",
    "  \n",
    "#   if s_end <= o_start:\n",
    "#     new_summary = summary[:s_start].text_with_ws + triple_swap[\"object\"] + summary[s_end:o_start].text_with_ws + triple_swap[\"subject\"] + summary[o_end:].text_with_ws\n",
    "#     augmentation_span = [(s_start, o_end - o_start + s_start-1), (o_end - s_end + s_start, o_end-1)]\n",
    "#   elif o_end <= s_start:\n",
    "#     new_summary = summary[:o_start].text_with_ws + triple_swap[\"subject\"] + summary[o_end:s_start].text_with_ws + triple_swap[\"object\"] + summary[s_end:].text_with_ws\n",
    "#     augmentation_span = [(o_start, s_end - s_start + o_start-1), (s_end - o_end + o_start-1, s_end-2)]\n",
    "#   else:\n",
    "#     return None, None\n",
    "#   new_summary = spacy(new_summary)\n",
    "#   if new_summary.text == summary.text:\n",
    "#     return None, None\n",
    "\n",
    "#   else:\n",
    "#     return new_summary, augmentation_span\n",
    "\n",
    "\n",
    "def s_o_swap(summary):\n",
    "  summary_ents = [ent for ent in summary.ents]\n",
    "  story_triples = []\n",
    "  document = client.annotate(summary.text, output_format='json')\n",
    "  triples = []\n",
    "  for sentence in document['sentences']:\n",
    "      for triple in sentence['openie']:\n",
    "          triples.append({\n",
    "            'subject': triple['subject'],\n",
    "            'relation': triple['relation'],\n",
    "              'object': triple['object']\n",
    "          })\n",
    "  if not triples:\n",
    "    return None, None\n",
    "  candidate_triples = []\n",
    "  for triple in triples:\n",
    "    subj = triple['subject']\n",
    "    obj = triple['object']\n",
    "    allow_triple = False\n",
    "    for ent in summary_ents:\n",
    "      if subj == ent.text or subj in ent.text or ent.text in subj:\n",
    "        allow_triple = True\n",
    "      if obj == ent.text or obj in ent.text or ent.text in obj:\n",
    "        allow_triple = True\n",
    "    if allow_triple == True:\n",
    "      candidate_triples.append(triple)\n",
    "  if not candidate_triples:\n",
    "    return None, None\n",
    "  triple_swap = random.choice(candidate_triples)\n",
    "  triple_swap[\"subject\"], s_start, s_end = openIE_filter(triple_swap[\"subject\"], summary)\n",
    "  triple_swap[\"object\"], o_start, o_end = openIE_filter(triple_swap[\"object\"], summary)\n",
    "  if s_start == 0:\n",
    "    triple_swap[\"object\"] = triple_swap[\"object\"].capitalize()\n",
    "    if summary[s_start].pos_ != \"PROPN\":\n",
    "      triple_swap[\"subject\"] = triple_swap[\"subject\"][0].lower() + triple_swap[\"subject\"][1:]\n",
    "  if o_start == 0:\n",
    "    triple_swap[\"subject\"] = triple_swap[\"subject\"].capitalize()\n",
    "    if summary[o_start].pos_ != \"PROPN\":\n",
    "      triple_swap[\"object\"] = triple_swap[\"object\"][0].lower() + triple_swap[\"object\"][1:]\n",
    "  if s_end <= o_start:\n",
    "    new_summary = summary[:s_start].text_with_ws + triple_swap[\"object\"] + summary[s_end:o_start].text_with_ws + triple_swap[\"subject\"] + summary[o_end:].text_with_ws\n",
    "    augmentation_span = [(s_start, o_end - o_start + s_start-1), (o_end - s_end + s_start, o_end-1)]\n",
    "  elif o_end <= s_start:\n",
    "    new_summary = summary[:o_start].text_with_ws + triple_swap[\"subject\"] + summary[o_end:s_start].text_with_ws + triple_swap[\"object\"] + summary[s_end:].text_with_ws\n",
    "    augmentation_span = [(o_start, s_end - s_start + o_start-1), (s_end - o_end + o_start-1, s_end-2)]\n",
    "  else:\n",
    "    return None, None\n",
    "  new_summary = spacy(new_summary)\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "SOURCE_LANG = \"en\"\n",
    "ACCEPTED_LANGS = [\"fr\", \"de\", \"zh-TW\", \"es\", \"ru\"]\n",
    "translator = Translator()\n",
    "\n",
    "def backtranslate(summary):\n",
    "  new_lang = random.choice(ACCEPTED_LANGS)\n",
    "  summary_trans = translator.translate(summary.text, dest=new_lang)\n",
    "  summary_btrans = translator.translate(summary_trans.text, dest=SOURCE_LANG)\n",
    "\n",
    "  new_summary = spacy(summary_btrans.text)\n",
    "  augmentation_span = (new_summary[0].i, new_summary[-1].i)\n",
    "\n",
    "  if summary.text == new_summary.text:\n",
    "    return None, None\n",
    "  else:\n",
    "    return new_summary, [augmentation_span]\n",
    "\n",
    "\n",
    "NOISE_PROB = 0.05\n",
    "DELETE_PROB = 0.8\n",
    "def addnoise(summary, augmentation_span):\n",
    "        summary_tokens = [token.text_with_ws for token in summary]\n",
    "\n",
    "        new_summary = []\n",
    "        for ix, token in enumerate(summary_tokens):\n",
    "            # don't modify text inside an augmented span\n",
    "            apply_augmentation = True\n",
    "            if augmentation_span:\n",
    "              for aug_span in augmentation_span:\n",
    "                if aug_span:\n",
    "                  span_start, span_end = aug_span\n",
    "                  if span_start <= ix <= span_end:\n",
    "                      apply_augmentation = False\n",
    "\n",
    "            # decide whether to add noise\n",
    "            if apply_augmentation and random.random() < NOISE_PROB:\n",
    "                # decide whether to replicate or delete token\n",
    "                if random.random() < DELETE_PROB:\n",
    "                    # update spans and skip token\n",
    "                    if augmentation_span:\n",
    "                      for el in range(0, len(augmentation_span)):\n",
    "                        aug_span = augmentation_span[el]\n",
    "                        if aug_span:\n",
    "                          span_start, span_end = aug_span\n",
    "                          if ix < span_start:\n",
    "                            span_start -= 1\n",
    "                            span_end -= 1\n",
    "                          aug_span = span_start, span_end\n",
    "                          augmentation_span[el] = aug_span\n",
    "                      if len(new_summary) > 0:\n",
    "                        if new_summary[-1][-1] != \" \":\n",
    "                          new_summary[-1] = new_summary[-1] + \" \"\n",
    "                      continue      \n",
    "                else:\n",
    "                  if augmentation_span:\n",
    "                    for el in range(0, len(augmentation_span)):\n",
    "                      aug_span = augmentation_span[el]\n",
    "                      if aug_span:\n",
    "                        span_start, span_end = aug_span\n",
    "                        if ix < span_start:\n",
    "                          span_start += 1\n",
    "                          span_end += 1\n",
    "                        aug_span = span_start, span_end\n",
    "                        augmentation_span[el] = aug_span\n",
    "\n",
    "                  new_summary.append(token)\n",
    "            new_summary.append(token)\n",
    "        new_summary = spacy(\"\".join(new_summary))\n",
    "\n",
    "        if summary.text == new_summary.text:\n",
    "            return None\n",
    "        else:\n",
    "            return new_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "# ids_to_keep = np.load('ids_to_keep.npy')\n",
    "xsum_filtered_train = load_from_disk('data/xsum_filtered/train')\n",
    "# xsum_filtered_train = xsum_filtered_train._data.filter(lambda x: x['keep']).remove_columns('keep')\n",
    "xsum_filtered_train._data = xsum_filtered_train._data.filter(xsum_filtered_train['keep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xsum_df = pd.DataFrame(xsum_corrupted_train)\n",
    "# !pip install install-jdk -t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 21:44:20 INFO: Writing properties to tmp file: corenlp_server-189c65bd380f4e8b.props\n",
      "2021-12-04 21:44:20 INFO: Starting server with command: java -Xmx5G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9002 -timeout 150000000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-189c65bd380f4e8b.props -annotators openie -preload -outputFormat serialized\n"
     ]
    }
   ],
   "source": [
    "#RUN THIS CELL TO START THE CORENLP TAGGER\n",
    "client = CoreNLPClient(timeout=150000000, be_quiet=True, annotators=['openie'], \n",
    "endpoint='http://localhost:9002')\n",
    "client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt update\n",
    "# !apt install default-jre\n",
    "# conda install -c anaconda openjdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "xsum_corrupted_train = xsum_filtered_train\n",
    "corrupted_summaries = xsum_corrupted_train['summary']\n",
    "already_done_tracker = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [15:34<00:00,  8.71it/s]\n",
      "100%|██████████| 8145/8145 [15:16<00:00,  8.89it/s]\n",
      "100%|██████████| 8145/8145 [16:07<00:00,  8.42it/s]\n",
      " 25%|██▌       | 2058/8145 [04:19<11:44,  8.64it/s]"
     ]
    }
   ],
   "source": [
    "perturbations = [\n",
    "#     \"s_o_swap\", \n",
    "#     \"bt\", \n",
    "    \"prn\", \n",
    "    \"dat\", \n",
    "    \"num\", \n",
    "    \"ent\", \n",
    "    \"neg\"\n",
    "]\n",
    "\n",
    "corrupt_percent = 0.3\n",
    "np.random.seed(2021)\n",
    "corrupt_indicies = np.random.choice(range(len(xsum_corrupted_train)), size=int(corrupt_percent*len(xsum_corrupted_train)))\n",
    "corruption_groups = np.array_split(corrupt_indicies, len(perturbations))\n",
    "\n",
    "total_corruption_count = 0\n",
    "iteration = 0\n",
    "\n",
    "#   client = CoreNLPClient(timeout=150000000, be_quiet=True, annotators=['openie'], \n",
    "#   endpoint='http://localhost:9002')\n",
    "#   client.start()\n",
    "\n",
    "for pert_type, indicies in zip(perturbations, corruption_groups):\n",
    "  for index in tqdm.tqdm(indicies):\n",
    "#     if index in already_done_tracker:\n",
    "#         continue\n",
    "    summary = spacy(xsum_corrupted_train[int(index)]['summary'])\n",
    "    article = spacy(xsum_corrupted_train[int(index)]['document'])\n",
    "#     summary = spacy(xsum_df['summary'][index])\n",
    "#     article = spacy(xsum_df['document'][index])\n",
    "#     try:\n",
    "#         if pert_type == \"s_o_swap\":\n",
    "#           new_summary, ags = s_o_swap(summary)\n",
    "    if pert_type == \"bt\":\n",
    "      new_summary, ags = backtranslate(summary)\n",
    "    if pert_type == \"prn\":\n",
    "      new_summary, ags = pronounswap(summary)\n",
    "    if pert_type == \"dat\":\n",
    "      new_summary, ags = dateswap(summary, article)\n",
    "    if pert_type == \"num\":\n",
    "      new_summary, ags = numberswap(summary, article)\n",
    "    if pert_type == \"ent\":\n",
    "      new_summary, ags = entityswap(summary, article)\n",
    "    if pert_type == \"neg\":\n",
    "      new_summary, ags = negation(summary)\n",
    "    if not new_summary and not ags:\n",
    "      new_summary = summary\n",
    "      ags = []\n",
    "#     except:\n",
    "#         new_summary = None\n",
    "    new_summary = addnoise(new_summary, ags)\n",
    "    if new_summary:\n",
    "      corrupted_summaries[index] = new_summary.text\n",
    "      total_corruption_count += 1\n",
    "    already_done_tracker.append(index)\n",
    "    \n",
    "# client.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_corrupted_train = xsum_corrupted_train.add_column('corrupted_summary', corrupted_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_corrupted_train.save_to_disk(\"data/xsum_baseline_corrupted/train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xsum_train_filtered.save_to_disk(\"data/xsum_filtered/train\")\n",
    "# xsum_val_filtered.save_to_disk(\"data/xsum_filtered/val\")\n",
    "# xsum_test_filtered.save_to_disk(\"data/xsum_filtered/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def backtranslate(summary):\n",
    "  new_lang = random.choice(ACCEPTED_LANGS)\n",
    "  summary_trans = translator.translate(summary, dest=new_lang)\n",
    "  summary_btrans = translator.translate(summary_trans.text, dest=SOURCE_LANG)\n",
    "\n",
    "  new_summary = summary_btrans\n",
    "  augmentation_span = (new_summary[0].i, new_summary[-1].i)\n",
    "\n",
    "  if summary == new_summary:\n",
    "    return None, None\n",
    "  else:\n",
    "    return new_summary, [augmentation_span]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_filtered_train = xsum_filtered_train.filter(lambda x: x['keep']).remove_columns('keep')\n",
    "xsum_filtered_val= xsum_filtered_train.filter(lambda x: x['keep']).remove_columns('keep')\n",
    "xsum_filtered_test= xsum_filtered_train.filter(lambda x: x['keep']).remove_columns('keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_corrupted_train = xsum_filtered_train\n",
    "corrupted_summaries = xsum_train_corrupted['summary']\n",
    "\n",
    "corrupt_percent = 0.4\n",
    "\n",
    "np.random.seed(2021)\n",
    "corrupt_indicies = np.random.choice(range(len(xsum_train_corrupted)), size=int(corrupt_percent*len(xsum_train_corrupted)))\n",
    "corruption_groups = np.split(corrupt_indicies, len(perturbations))\n",
    "\n",
    "total_corruption_count = 0\n",
    "for pert_type, indicies in zip(perturbations, corruption_groups):\n",
    "  for index in indicies:\n",
    "    if pert_type == \"bt\":\n",
    "      new_summary, ags = backtranslate(summary)\n",
    "    if pert_type == \"prn\":\n",
    "      new_summary, ags = pronounswap(summary)\n",
    "    if pert_type == \"dat\":\n",
    "      new_summary, ags = dateswap(summary, article)\n",
    "    if pert_type == \"num\":\n",
    "      new_summary, ags = numberswap(summary, article)\n",
    "    if pert_type == \"ent\":\n",
    "      new_summary, ags = entityswap(summary, article)\n",
    "    if pert_type == \"neg\":\n",
    "      new_summary, ags = negation(summary)\n",
    "    if not new_summary and not ags:\n",
    "      new_summary = summary\n",
    "      ags = []\n",
    "    new_summary = addnoise(new_summary, ags)\n",
    "    if new_summary:\n",
    "      corrupted_summaries[index] = new_summary.text\n",
    "      total_corruption_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
