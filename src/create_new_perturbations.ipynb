{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: anytree in /opt/conda/lib/python3.7/site-packages (2.8.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from anytree) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: OpenHowNet in /opt/conda/lib/python3.7/site-packages (1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: googletrans==4.0.0-rc1 in /opt/conda/lib/python3.7/site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in /opt/conda/lib/python3.7/site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: httpcore==0.9.* in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2020.6.20)\n",
      "Requirement already satisfied: chardet==3.* in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: hstspreload in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.12.1)\n",
      "Requirement already satisfied: idna==2.* in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.8)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /opt/conda/lib/python3.7/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in /opt/conda/lib/python3.7/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /opt/conda/lib/python3.7/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /opt/conda/lib/python3.7/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 01:42:10 WARNING: Directory ./corenlp already exists. Please install CoreNLP to a new directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoreNLP-to-HTML.xsl\n",
      "LIBRARY-LICENSES\n",
      "LICENSE.txt\n",
      "Makefile\n",
      "README.txt\n",
      "RESOURCE-LICENSES\n",
      "SemgrexDemo.java\n",
      "ShiftReduceDemo.java\n",
      "StanfordCoreNlpDemo.java\n",
      "StanfordDependenciesManual.pdf\n",
      "build.xml\n",
      "corenlp.sh\n",
      "ejml-core-0.39-sources.jar\n",
      "ejml-core-0.39.jar\n",
      "ejml-ddense-0.39-sources.jar\n",
      "ejml-ddense-0.39.jar\n",
      "ejml-simple-0.39-sources.jar\n",
      "ejml-simple-0.39.jar\n",
      "input.txt\n",
      "input.txt.out\n",
      "input.txt.xml\n",
      "istack-commons-runtime-3.0.7-sources.jar\n",
      "istack-commons-runtime-3.0.7.jar\n",
      "javax.activation-api-1.2.0-sources.jar\n",
      "javax.activation-api-1.2.0.jar\n",
      "javax.json-api-1.0-sources.jar\n",
      "javax.json.jar\n",
      "jaxb-api-2.4.0-b180830.0359-sources.jar\n",
      "jaxb-api-2.4.0-b180830.0359.jar\n",
      "jaxb-impl-2.4.0-b180830.0438-sources.jar\n",
      "jaxb-impl-2.4.0-b180830.0438.jar\n",
      "joda-time-2.10.5-sources.jar\n",
      "joda-time.jar\n",
      "jollyday-0.4.9-sources.jar\n",
      "jollyday.jar\n",
      "patterns\n",
      "pom-java-11.xml\n",
      "pom-java-17.xml\n",
      "pom.xml\n",
      "protobuf-java-3.11.4.jar\n",
      "slf4j-api.jar\n",
      "slf4j-simple.jar\n",
      "stanford-corenlp-4.3.2-javadoc.jar\n",
      "stanford-corenlp-4.3.2-models.jar\n",
      "stanford-corenlp-4.3.2-sources.jar\n",
      "stanford-corenlp-4.3.2.jar\n",
      "sutime\n",
      "tokensregex\n",
      "xom-1.3.7-sources.jar\n",
      "xom.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openhownet_data.zip:  99%|█████████▉| 106432/107432.51 [00:15<00:00, 8452.32KB/s]/opt/conda/lib/python3.7/site-packages/tqdm/std.py:536: TqdmWarning: clamping frac to range [0, 1]\n",
      "  colour=colour)\n",
      "openhownet_data.zip: 100%|██████████| 107433/107432.51 [00:15<00:00, 6777.76KB/s]\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers sentence-transformers stanza datasets rouge_score spacy\n",
    "# ! python -m spacy download en_core_web_lg\n",
    "!pip install anytree\n",
    "!pip install OpenHowNet\n",
    "!pip install googletrans==4.0.0-rc1\n",
    "\n",
    "import stanza\n",
    "\n",
    "corenlp_dir = './corenlp'\n",
    "stanza.install_corenlp(dir=corenlp_dir)\n",
    "\n",
    "# Set the CORENLP_HOME environment variable to point to the installation location\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
    "\n",
    "!ls $CORENLP_HOME\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "import pandas as pd\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "\n",
    "import random\n",
    "import spacy\n",
    "spacy = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "import random\n",
    "import OpenHowNet\n",
    "OpenHowNet.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_ws(old_token, new_token):\n",
    "    # Align trailing whitespaces between tokens\n",
    "    if old_token[-1] == new_token[-1] == \" \":\n",
    "        return new_token\n",
    "    elif old_token[-1] == \" \":\n",
    "        return new_token + \" \"\n",
    "    elif new_token[-1] == \" \":\n",
    "        return new_token[:-1]\n",
    "    else:\n",
    "        return new_token\n",
    "    \n",
    "NEGATABLE_TOKENS = (\"are\", \"is\", \"was\", \"were\", \"have\", \"has\", \"had\",\n",
    "                                   \"do\", \"does\", \"did\", \"can\", \"ca\", \"could\", \"may\",\n",
    "                                   \"might\", \"must\", \"shall\", \"should\", \"will\", \"would\")\n",
    "\n",
    "\n",
    "\n",
    "def negation(summary):\n",
    "  candidate_tokens = [token for token in summary if token.text in NEGATABLE_TOKENS]\n",
    "\n",
    "  if not candidate_tokens:\n",
    "      return None, None\n",
    "\n",
    "  # choose random token to negate\n",
    "  negated_token = random.choice(candidate_tokens)\n",
    "  negated_index = negated_token.i\n",
    "  L = len(summary)\n",
    "\n",
    "  # negation occurs at the first negatable token (e.g. does not have)\n",
    "  if negated_index > 0:\n",
    "    if summary[negated_index-1].text in NEGATABLE_TOKENS:\n",
    "      negated_token = summary[negated_index-1]\n",
    "      negated_index = negated_index-1\n",
    "  \n",
    "  #check whether qualified by a negative\n",
    "  is_negative = False\n",
    "  if (L-1) > negated_index:\n",
    "    if summary[negated_index + 1].text in [\"not\", \"n't\"]:\n",
    "      is_negative = True\n",
    "    elif summary[negated_index+1].text == \"no\":\n",
    "      return None, None\n",
    "\n",
    "  #add not/n't if is_negative is False, remove if True\n",
    "  tokens = [token.text_with_ws for token in summary]\n",
    "  if is_negative:\n",
    "      if summary[negated_index + 1].text.lower() == \"n't\":\n",
    "          if summary[negated_index + 1].text.lower() == \"ca\":\n",
    "              tokens[negated_index] = \"can\" if tokens[negated_index].islower() else \"Can\"\n",
    "          tokens[negated_index] = tokens[negated_index] + \" \"\n",
    "      tokens.pop(negated_index + 1)\n",
    "\n",
    "  else:\n",
    "      if summary[negated_index].text.lower() in [\"am\", \"may\", \"might\", \"must\", \"shall\", \"will\"]:\n",
    "          negation = \"not \"\n",
    "      else:\n",
    "          negation = random.choice([\"not \", \"n't \"])\n",
    "\n",
    "      if negation == \"n't \":\n",
    "          if summary[negated_index].text.lower() == \"can\":\n",
    "              tokens[negated_index] = \"ca\" if tokens[negated_index].islower() else \"Ca\"\n",
    "          else:\n",
    "              tokens[negated_index] = tokens[negated_index][:-1]\n",
    "      tokens.insert(negated_index + 1, negation)\n",
    "  \n",
    "  new_summary = spacy(\"\".join(tokens))\n",
    "  augmentation_span = [(negated_index, negated_index if is_negative else negated_index + 1)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "CLASS_TO_PRONOUN = {\n",
    "            \"SUBJECT\": [\"you\", \"he\", \"she\", \"we\", \"they\"],\n",
    "            \"OBJECT\": [\"me\", \"you\", \"him\", \"her\", \"us\", \"them\"],\n",
    "            \"POSSESSIVE\": [\"my\", \"your\", \"his\", \"her\", \"its\", \"our\", \"your\", \"their\"],\n",
    "            \"REFLEXIVE\": [\"myself\", \"yourself\", \"himself\", \"itself\", \"ourselves\", \"yourselves\", \"themselves\"]\n",
    "        }\n",
    "\n",
    "\n",
    "PRONOUN_TO_CLASS = {pronoun: key for (key, values) in CLASS_TO_PRONOUN.items() for pronoun in values}\n",
    "PRONOUNS = [pronoun for pronoun in PRONOUN_TO_CLASS.keys()]\n",
    "\n",
    "\n",
    "def pronounswap(summary):\n",
    "  summary_pronouns = [token for token in summary if token.text.lower() in PRONOUNS]\n",
    "\n",
    "  if not summary_pronouns:\n",
    "    return None, None\n",
    "\n",
    "  swap = random.choice(summary_pronouns)\n",
    "  swap_index = swap.i\n",
    "  swap_class = PRONOUN_TO_CLASS[swap.text.lower()]\n",
    "\n",
    "  candidate_tokens = [token for token in CLASS_TO_PRONOUN[swap_class] if token != swap.text.lower()]\n",
    "\n",
    "  if not candidate_tokens:\n",
    "    return None, None\n",
    "\n",
    "  swapped_token = random.choice(candidate_tokens)\n",
    "  swapped_token = align_ws(swap.text_with_ws, swapped_token)\n",
    "  swapped_token = swapped_token if swap.text.islower() else swapped_token.capitalize()\n",
    "\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  summary_tokens[swap_index] = swapped_token\n",
    "\n",
    "  new_summary = spacy(\"\".join(summary_tokens))\n",
    "\n",
    "  augmentation_span = [(swap_index, swap_index)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "\n",
    "ENTITY_CATEGORIES = (\"PERSON\", \"ORG\", \"NORP\", \"FAC\", \"GPE\", \"LOC\", \"PRODUCT\",\n",
    "                           \"WORK_OF_ART\", \"EVENT\")\n",
    "\n",
    "def entityswap(summary, source):\n",
    "  source_ents = [ent for ent in source.ents if ent.label_ in ENTITY_CATEGORIES]\n",
    "  summary_ents = [ent for ent in summary.ents if ent.label_ in ENTITY_CATEGORIES]\n",
    "\n",
    "  if not source_ents or not summary_ents:\n",
    "    return None, None\n",
    "\n",
    "  swap = random.choice(summary_ents)\n",
    "  candidate_ents = [ent for ent in source_ents if ent.text != swap.text and ent.text not in swap.text and swap.text not in ent.text]\n",
    "\n",
    "  if not candidate_ents:\n",
    "    return None, None\n",
    "\n",
    "  swapped_ent = random.choice(candidate_ents)\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  swapped_token = align_ws(swap.text_with_ws, swapped_ent.text_with_ws)\n",
    "  summary_tokens = summary_tokens[:swap.start] + [swapped_token] + summary_tokens[swap.end:]\n",
    "\n",
    "  new_summary = spacy(\"\".join(summary_tokens))\n",
    "  augmentation_span = [(swap.start, swap.start + len(swapped_ent) - 1)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "NUMBER_CATEGORIES = (\"PERCENT\", \"MONEY\", \"QUANTITY\", \"CARDINAL\")\n",
    "\n",
    "def numberswap(summary, source):\n",
    "  source_ents = [ent for ent in source.ents if ent.label_ in NUMBER_CATEGORIES]\n",
    "  summary_ents = [ent for ent in summary.ents if ent.label_ in NUMBER_CATEGORIES]\n",
    "\n",
    "  if not source_ents or not summary_ents:\n",
    "    return None, None\n",
    "\n",
    "  swap = random.choice(summary_ents)\n",
    "  candidate_ents = [ent for ent in source_ents if ent.text != swap.text and ent.text not in swap.text and swap.text not in ent.text]\n",
    "\n",
    "  if not candidate_ents:\n",
    "    return None, None\n",
    "\n",
    "  swapped_ent = random.choice(candidate_ents)\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  swapped_token = align_ws(swap.text_with_ws, swapped_ent.text_with_ws)\n",
    "  summary_tokens = summary_tokens[:swap.start] + [swapped_token] + summary_tokens[swap.end:]\n",
    "\n",
    "  new_summary = spacy(\"\".join(summary_tokens))\n",
    "  augmentation_span = [(swap.start, swap.start + len(swapped_ent) - 1)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "DATE_CATEGORIES = (\"DATE\", \"TIME\")\n",
    "\n",
    "def dateswap(summary, source):\n",
    "  source_ents = [ent for ent in source.ents if ent.label_ in DATE_CATEGORIES]\n",
    "  summary_ents = [ent for ent in summary.ents if ent.label_ in DATE_CATEGORIES]\n",
    "\n",
    "  if not source_ents or not summary_ents:\n",
    "    return None, None\n",
    "\n",
    "  swap = random.choice(summary_ents)\n",
    "  candidate_ents = [ent for ent in source_ents if ent.text != swap.text and ent.text not in swap.text and swap.text not in ent.text]\n",
    "\n",
    "  if not candidate_ents:\n",
    "    return None, None\n",
    "\n",
    "  swapped_ent = random.choice(candidate_ents)\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  swapped_token = align_ws(swap.text_with_ws, swapped_ent.text_with_ws)\n",
    "  summary_tokens = summary_tokens[:swap.start] + [swapped_token] + summary_tokens[swap.end:]\n",
    "\n",
    "  new_summary = spacy(\"\".join(summary_tokens))\n",
    "  augmentation_span = [(swap.start, swap.start + len(swapped_ent) - 1)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "\n",
    "POS_TAGS = (\"NOUN\", \"ADJ\", \"PROPN\")\n",
    "\n",
    "\n",
    "# def openIE_filter(extract, summary):\n",
    "#   new_text = \"\"\n",
    "#   extract = spacy(extract)\n",
    "\n",
    "#   start_index = 0\n",
    "#   for token in summary:\n",
    "#     if token.text != extract[0].text:\n",
    "#       start_index+=1\n",
    "#     else:\n",
    "#       break\n",
    "#   tracking_index = start_index\n",
    "#   for word in extract:\n",
    "#     while (summary[tracking_index].text != word.text):\n",
    "#       tracking_index+=1\n",
    "#     if summary[tracking_index].pos_ in POS_TAGS:\n",
    "#       break\n",
    "#   start_index = tracking_index\n",
    "\n",
    "#   while (summary[tracking_index].pos_ != \"NOUN\" and summary[tracking_index].pos_ != \"PROPN\"):\n",
    "#     new_text+=(summary[tracking_index].text_with_ws)\n",
    "#     tracking_index += 1\n",
    "#   while (summary[tracking_index].pos_ == \"NOUN\" or summary[tracking_index].pos_ == \"PROPN\"):\n",
    "#     new_text+=(summary[tracking_index].text_with_ws)\n",
    "#     tracking_index += 1\n",
    "#   return new_text, start_index, tracking_index\n",
    "\n",
    "def openIE_filter(extract, summary):\n",
    "  new_text = \"\"\n",
    "  extract = spacy(extract)\n",
    "  start_index = 0\n",
    "  for token in summary:\n",
    "    if token.text != extract[0].text:\n",
    "      start_index+=1\n",
    "    else:\n",
    "      break\n",
    "  tracking_index = start_index\n",
    "  for word in extract:\n",
    "    while (summary[tracking_index].text != word.text):\n",
    "      tracking_index+=1\n",
    "    if summary[tracking_index].pos_ in POS_TAGS:\n",
    "      break\n",
    "  start_index = tracking_index\n",
    "  while (summary[tracking_index].pos_ != \"NOUN\" and summary[tracking_index].pos_ != \"PROPN\" and tracking_index < len(summary)):\n",
    "    new_text+=(summary[tracking_index].text_with_ws)\n",
    "    tracking_index += 1\n",
    "  if len(summary) == tracking_index:\n",
    "    return new_text, start_index, tracking_index\n",
    "  while (summary[tracking_index].pos_ == \"NOUN\" or summary[tracking_index].pos_ == \"PROPN\"):\n",
    "    new_text+=(summary[tracking_index].text_with_ws)\n",
    "    tracking_index += 1\n",
    "  return new_text, start_index, tracking_index\n",
    "\n",
    "\n",
    "# def s_o_swap(summary):\n",
    "#   summary_ents = [ent for ent in summary.ents]\n",
    "#   client = CoreNLPClient(timeout=150000000, be_quiet=True, annotators=['openie'], \n",
    "#   endpoint='http://localhost:9002')\n",
    "#   client.start()\n",
    "#   import time\n",
    "#   time.sleep(2)\n",
    "#   story_triples = []\n",
    "\n",
    "#   document = client.annotate(summary.text, output_format='json')\n",
    "#   triples = []\n",
    "#   for sentence in document['sentences']:\n",
    "#       for triple in sentence['openie']:\n",
    "#           triples.append({\n",
    "#             'subject': triple['subject'],\n",
    "#             'relation': triple['relation'],\n",
    "#               'object': triple['object']\n",
    "#           })\n",
    "  \n",
    "#   client.stop()\n",
    "#   time.sleep(2)\n",
    "\n",
    "#   if not triples:\n",
    "#     return None, None\n",
    "#   candidate_triples = []\n",
    "#   for triple in triples:\n",
    "#     subj = triple['subject']\n",
    "#     obj = triple['object']\n",
    "#     allow_triple = False\n",
    "#     for ent in summary_ents:\n",
    "#       if subj == ent.text or subj in ent.text or ent.text in subj:\n",
    "#         allow_triple = True\n",
    "#       if obj == ent.text or obj in ent.text or ent.text in obj:\n",
    "#         allow_triple = True\n",
    "#     if allow_triple == True:\n",
    "#       candidate_triples.append(triple)\n",
    "#   if not candidate_triples:\n",
    "#     return None, None\n",
    "#   triple_swap = random.choice(candidate_triples)\n",
    "#   triple_swap[\"subject\"], s_start, s_end = openIE_filter(triple_swap[\"subject\"], summary)\n",
    "#   triple_swap[\"object\"], o_start, o_end = openIE_filter(triple_swap[\"object\"], summary)\n",
    "#   if s_start == 0:\n",
    "#     triple_swap[\"object\"] = triple_swap[\"object\"].capitalize()\n",
    "#     if summary[s_start].pos_ != \"PROPN\":\n",
    "#       triple_swap[\"subject\"] = triple_swap[\"subject\"][0].lower() + triple_swap[\"subject\"][1:]\n",
    "#   if o_start == 0:\n",
    "#     triple_swap[\"subject\"] = triple_swap[\"subject\"].capitalize()\n",
    "#     if summary[o_start].pos_ != \"PROPN\":\n",
    "#       triple_swap[\"object\"] = triple_swap[\"object\"][0].lower() + triple_swap[\"object\"][1:]\n",
    "  \n",
    "#   if s_end <= o_start:\n",
    "#     new_summary = summary[:s_start].text_with_ws + triple_swap[\"object\"] + summary[s_end:o_start].text_with_ws + triple_swap[\"subject\"] + summary[o_end:].text_with_ws\n",
    "#     augmentation_span = [(s_start, o_end - o_start + s_start-1), (o_end - s_end + s_start, o_end-1)]\n",
    "#   elif o_end <= s_start:\n",
    "#     new_summary = summary[:o_start].text_with_ws + triple_swap[\"subject\"] + summary[o_end:s_start].text_with_ws + triple_swap[\"object\"] + summary[s_end:].text_with_ws\n",
    "#     augmentation_span = [(o_start, s_end - s_start + o_start-1), (s_end - o_end + o_start-1, s_end-2)]\n",
    "#   else:\n",
    "#     return None, None\n",
    "#   new_summary = spacy(new_summary)\n",
    "#   if new_summary.text == summary.text:\n",
    "#     return None, None\n",
    "\n",
    "#   else:\n",
    "#     return new_summary, augmentation_span\n",
    "\n",
    "\n",
    "def s_o_swap(summary):\n",
    "  summary_ents = [ent for ent in summary.ents]\n",
    "  story_triples = []\n",
    "  document = client.annotate(summary.text, output_format='json')\n",
    "  triples = []\n",
    "  for sentence in document['sentences']:\n",
    "      for triple in sentence['openie']:\n",
    "          triples.append({\n",
    "            'subject': triple['subject'],\n",
    "            'relation': triple['relation'],\n",
    "              'object': triple['object']\n",
    "          })\n",
    "  if not triples:\n",
    "    return None, None\n",
    "  candidate_triples = []\n",
    "  for triple in triples:\n",
    "    subj = triple['subject']\n",
    "    obj = triple['object']\n",
    "    allow_triple = False\n",
    "    for ent in summary_ents:\n",
    "      if subj == ent.text or subj in ent.text or ent.text in subj:\n",
    "        allow_triple = True\n",
    "      if obj == ent.text or obj in ent.text or ent.text in obj:\n",
    "        allow_triple = True\n",
    "    if allow_triple == True:\n",
    "      candidate_triples.append(triple)\n",
    "  if not candidate_triples:\n",
    "    return None, None\n",
    "  triple_swap = random.choice(candidate_triples)\n",
    "  triple_swap[\"subject\"], s_start, s_end = openIE_filter(triple_swap[\"subject\"], summary)\n",
    "  triple_swap[\"object\"], o_start, o_end = openIE_filter(triple_swap[\"object\"], summary)\n",
    "  if s_start == 0:\n",
    "    triple_swap[\"object\"] = triple_swap[\"object\"].capitalize()\n",
    "    if summary[s_start].pos_ != \"PROPN\":\n",
    "      triple_swap[\"subject\"] = triple_swap[\"subject\"][0].lower() + triple_swap[\"subject\"][1:]\n",
    "  if o_start == 0:\n",
    "    triple_swap[\"subject\"] = triple_swap[\"subject\"].capitalize()\n",
    "    if summary[o_start].pos_ != \"PROPN\":\n",
    "      triple_swap[\"object\"] = triple_swap[\"object\"][0].lower() + triple_swap[\"object\"][1:]\n",
    "  if s_end <= o_start:\n",
    "    new_summary = summary[:s_start].text_with_ws + triple_swap[\"object\"] + summary[s_end:o_start].text_with_ws + triple_swap[\"subject\"] + summary[o_end:].text_with_ws\n",
    "    augmentation_span = [(s_start, o_end - o_start + s_start-1), (o_end - s_end + s_start, o_end-1)]\n",
    "  elif o_end <= s_start:\n",
    "    new_summary = summary[:o_start].text_with_ws + triple_swap[\"subject\"] + summary[o_end:s_start].text_with_ws + triple_swap[\"object\"] + summary[s_end:].text_with_ws\n",
    "    augmentation_span = [(o_start, s_end - s_start + o_start-1), (s_end - o_end + o_start-1, s_end-2)]\n",
    "  else:\n",
    "    return None, None\n",
    "  new_summary = spacy(new_summary)\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "SOURCE_LANG = \"en\"\n",
    "ACCEPTED_LANGS = [\"fr\", \"de\", \"zh-TW\", \"es\", \"ru\"]\n",
    "translator = Translator()\n",
    "\n",
    "def backtranslate(summary):\n",
    "  new_lang = random.choice(ACCEPTED_LANGS)\n",
    "  summary_trans = translator.translate(summary.text, dest=new_lang)\n",
    "  summary_btrans = translator.translate(summary_trans.text, dest=SOURCE_LANG)\n",
    "\n",
    "  new_summary = spacy(summary_btrans.text)\n",
    "  augmentation_span = (new_summary[0].i, new_summary[-1].i)\n",
    "\n",
    "  if summary.text == new_summary.text:\n",
    "    return None, None\n",
    "  else:\n",
    "    return new_summary, [augmentation_span]\n",
    "\n",
    "\n",
    "NOISE_PROB = 0.05\n",
    "DELETE_PROB = 0.8\n",
    "def addnoise(summary, augmentation_span):\n",
    "        summary_tokens = [token.text_with_ws for token in summary]\n",
    "\n",
    "        new_summary = []\n",
    "        for ix, token in enumerate(summary_tokens):\n",
    "            # don't modify text inside an augmented span\n",
    "            apply_augmentation = True\n",
    "            if augmentation_span:\n",
    "              for aug_span in augmentation_span:\n",
    "                if aug_span:\n",
    "                  span_start, span_end = aug_span\n",
    "                  if span_start <= ix <= span_end:\n",
    "                      apply_augmentation = False\n",
    "\n",
    "            # decide whether to add noise\n",
    "            if apply_augmentation and random.random() < NOISE_PROB:\n",
    "                # decide whether to replicate or delete token\n",
    "                if random.random() < DELETE_PROB:\n",
    "                    # update spans and skip token\n",
    "                    if augmentation_span:\n",
    "                      for el in range(0, len(augmentation_span)):\n",
    "                        aug_span = augmentation_span[el]\n",
    "                        if aug_span:\n",
    "                          span_start, span_end = aug_span\n",
    "                          if ix < span_start:\n",
    "                            span_start -= 1\n",
    "                            span_end -= 1\n",
    "                          aug_span = span_start, span_end\n",
    "                          augmentation_span[el] = aug_span\n",
    "                      if len(new_summary) > 0:\n",
    "                        if new_summary[-1][-1] != \" \":\n",
    "                          new_summary[-1] = new_summary[-1] + \" \"\n",
    "                      continue      \n",
    "                else:\n",
    "                  if augmentation_span:\n",
    "                    for el in range(0, len(augmentation_span)):\n",
    "                      aug_span = augmentation_span[el]\n",
    "                      if aug_span:\n",
    "                        span_start, span_end = aug_span\n",
    "                        if ix < span_start:\n",
    "                          span_start += 1\n",
    "                          span_end += 1\n",
    "                        aug_span = span_start, span_end\n",
    "                        augmentation_span[el] = aug_span\n",
    "\n",
    "                  new_summary.append(token)\n",
    "            new_summary.append(token)\n",
    "        new_summary = spacy(\"\".join(new_summary))\n",
    "\n",
    "        if summary.text == new_summary.text:\n",
    "            return None\n",
    "        else:\n",
    "            return new_summary\n",
    "        \n",
    "def predicateswap(summary, source):\n",
    "  source_verbs = [token for token in source if token.pos_ == \"VERB\"]\n",
    "  source_adverbs = [token for token in source if token.pos_ == \"ADV\"]\n",
    "  summary_preds = [token for token in summary if token.pos_ == \"VERB\" or token.pos_ == \"ADV\"]\n",
    "  if not summary_preds or not source_adverbs or not source_verbs:\n",
    "    return None, None\n",
    "  original = random.choice(summary_preds)\n",
    "  ix = original.i\n",
    "  if original.pos_ == \"VERB\":\n",
    "    new = random.choice(source_verbs)\n",
    "  else:\n",
    "    new = random.choice(source_adverbs)\n",
    "\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  summary_tokens[ix] = new.text_with_ws\n",
    "  new_summary = \"\".join(summary_tokens)\n",
    "  new_summary = spacy(new_summary)\n",
    "  augmentation_span = (ix, ix)\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "  else:\n",
    "    return new_summary, [augmentation_span]\n",
    "\n",
    "\n",
    "PERTURBABLE = (\"ADJ\", \"ADV\", \"VERB\", \"ADP\", \"CONJ\")\n",
    "\n",
    "def sem_pert(word, N):\n",
    "  translated_word = translator.translate(word, dest='zh-cn')\n",
    "  translated_word = translated_word.text\n",
    "  for i in range(0, N):\n",
    "    query_result = hownet_dict_advanced.get_nearest_words_via_sememes(translated_word,15)\n",
    "    if not query_result:\n",
    "      return None\n",
    "    translated_word = random.choice((random.choice(query_result))['synset'])['word']\n",
    "  replacement_word = translator.translate(translated_word, dest='en')\n",
    "  if replacement_word == translated_word:\n",
    "    return None\n",
    "  return replacement_word.text\n",
    "\n",
    "hownet_dict_advanced = OpenHowNet.HowNetDict(use_sim=True)\n",
    "\n",
    "def smartswap(summary):\n",
    "  summary_ents = [ent for ent in summary.ents]\n",
    "  if not summary_ents[1:]:\n",
    "    return None, None\n",
    "  \n",
    "  document = client.annotate(summary.text, output_format='json')\n",
    "  triples = []\n",
    "  for sentence in document['sentences']:\n",
    "      for triple in sentence['openie']:\n",
    "          triples.append({\n",
    "            'subject': triple['subject'],\n",
    "            'relation': triple['relation'],\n",
    "              'object': triple['object']\n",
    "          })\n",
    "  \n",
    "  \n",
    "  if not triples:\n",
    "    return None, None\n",
    "  candidate_triples = []\n",
    "  for triple in triples:\n",
    "    subj = triple['subject']\n",
    "    obj = triple['object']\n",
    "    allow_triple = False\n",
    "    for ent in summary_ents:\n",
    "      if subj == ent.text or subj in ent.text or ent.text in subj:\n",
    "        allow_triple = True\n",
    "      if obj == ent.text or obj in ent.text or ent.text in obj:\n",
    "        allow_triple = True\n",
    "    if allow_triple == True:\n",
    "      candidate_triples.append(triple)\n",
    "  if not candidate_triples:\n",
    "    return None, None\n",
    "  triple_pert = random.choice(candidate_triples)\n",
    "  triple_rel = spacy(triple_pert[\"relation\"])\n",
    "  triple_rel = [token.text for token in triple_rel]\n",
    "  \n",
    "  target_tokens = [token for token in summary if token.text in triple_rel and token.pos_ in PERTURBABLE]\n",
    "  if not target_tokens:\n",
    "    return None, None\n",
    "  if len(target_tokens) > 4:\n",
    "    target_tokens = random.sample(target_tokens, 4)\n",
    "  indices = [token.i for token in target_tokens]\n",
    "  replacements = []\n",
    "  for word in target_tokens:\n",
    "    new = sem_pert(word.text, random.randint(1, 4))\n",
    "    if new[0].isupper:\n",
    "      new = new[0].lower() + new[1:]\n",
    "    replacements.append(spacy(new))\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  for ix in range(0, len(indices)):\n",
    "    summary_tokens[indices[ix]] = replacements[ix].text_with_ws + \" \"\n",
    "  new_summary = \"\".join(summary_tokens)\n",
    "\n",
    "  new_summary = spacy(new_summary)\n",
    "  augmentation_span = []\n",
    "  for ix in indices:\n",
    "    augmentation_span.append((ix, ix))\n",
    "  if summary.text == new_summary.text:\n",
    "      return None, None\n",
    "  else:\n",
    "      return new_summary, augmentation_span\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "# ids_to_keep = np.load('ids_to_keep.npy')\n",
    "xsum_filtered_train = load_from_disk('data/xsum_filtered/train')\n",
    "xsum_filtered_train._data = xsum_filtered_train._data.filter(xsum_filtered_train['keep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False\n",
    "if debug:\n",
    "    client = CoreNLPClient(timeout=150000000, be_quiet=True, annotators=['openie'], \n",
    "    endpoint='http://localhost:9002')\n",
    "    client.start()\n",
    "    print(smartswap(spacy(xsum_filtered_train['summary'][0])))\n",
    "    client.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xsum_df = pd.DataFrame(xsum_corrupted_train)\n",
    "# !pip install install-jdk -t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 01:42:53 INFO: Writing properties to tmp file: corenlp_server-4fc419d58f604c0a.props\n",
      "2021-12-05 01:42:53 INFO: Starting server with command: java -Xmx5G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9002 -timeout 150000000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-4fc419d58f604c0a.props -annotators openie -preload -outputFormat serialized\n"
     ]
    }
   ],
   "source": [
    "#RUN THIS CELL TO START THE CORENLP TAGGER\n",
    "client = CoreNLPClient(timeout=150000000, be_quiet=True, annotators=['openie'], \n",
    "endpoint='http://localhost:9002')\n",
    "client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt update\n",
    "# !apt install default-jre\n",
    "# conda install -c anaconda openjdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "xsum_corrupted_train = xsum_filtered_train\n",
    "# corrupted_summaries = xsum_corrupted_train['summary']\n",
    "already_done_tracker = []\n",
    "corrupted_summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption: predicate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1782/12067 [00:09<01:13, 140.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate, save_index: 100 - SAVED!\n",
      "10744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3208/12067 [00:20<01:31, 97.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate, save_index: 200 - SAVED!\n",
      "10781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 4601/12067 [00:29<00:35, 209.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate, save_index: 300 - SAVED!\n",
      "10815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 5954/12067 [00:39<01:17, 78.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate, save_index: 400 - SAVED!\n",
      "10851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 7169/12067 [00:49<00:42, 115.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate, save_index: 500 - SAVED!\n",
      "10887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 8705/12067 [01:00<00:23, 142.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate, save_index: 600 - SAVED!\n",
      "10926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 10124/12067 [01:10<00:08, 232.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate, save_index: 700 - SAVED!\n",
      "10958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 11589/12067 [01:20<00:04, 98.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate, save_index: 800 - SAVED!\n",
      "10986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12067/12067 [01:27<00:00, 137.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption done and saved to: data/corruption_dict/predicate.pkl\n",
      "Total corruption_count = 301\n",
      "Corruption: s_o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 101/12067 [00:16<27:08,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 100 - SAVED!\n",
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 201/12067 [00:33<32:38,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 200 - SAVED!\n",
      "83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 301/12067 [00:49<30:28,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 300 - SAVED!\n",
      "127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 349/12067 [00:58<30:03,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 26, max length 26.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 372/12067 [01:01<24:02,  8.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 13, max length 13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 401/12067 [01:06<28:57,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 400 - SAVED!\n",
      "168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 454/12067 [01:14<25:05,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 24, max length 24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 500/12067 [01:22<30:15,  6.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 20, max length 20.\n",
      "s_o, save_index: 500 - SAVED!\n",
      "210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 531/12067 [01:27<41:12,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 29, max length 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 586/12067 [01:36<30:47,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 17, max length 17.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 601/12067 [01:39<38:36,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 600 - SAVED!\n",
      "252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 624/12067 [01:42<28:54,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 21, max length 21.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 633/12067 [01:44<35:27,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 27, max length 27.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 701/12067 [01:54<27:03,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 700 - SAVED!\n",
      "289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 779/12067 [02:07<27:45,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 16, max length 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 782/12067 [02:08<28:17,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 13, max length 13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 791/12067 [02:09<30:11,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 24, max length 24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 801/12067 [02:11<22:59,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 800 - SAVED!\n",
      "340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 901/12067 [02:27<32:27,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 900 - SAVED!\n",
      "376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1000/12067 [02:43<25:49,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 1000 - SAVED!\n",
      "413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1101/12067 [02:59<27:49,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 1100 - SAVED!\n",
      "452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1126/12067 [03:03<26:45,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 17, max length 17.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1200/12067 [03:15<31:40,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 1200 - SAVED!\n",
      "486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1300/12067 [03:31<25:11,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 1300 - SAVED!\n",
      "520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1401/12067 [03:47<25:25,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 1400 - SAVED!\n",
      "563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1409/12067 [03:48<22:47,  7.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 21, max length 21.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1449/12067 [03:54<26:25,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 25, max length 25.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1501/12067 [04:02<23:40,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 1500 - SAVED!\n",
      "594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1600/12067 [04:17<33:11,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 1600 - SAVED!\n",
      "637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1697/12067 [04:32<24:47,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 27, max length 27.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1701/12067 [04:33<25:13,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 1700 - SAVED!\n",
      "679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1800/12067 [04:49<30:38,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 1800 - SAVED!\n",
      "721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1844/12067 [04:56<35:07,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 24, max length 24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1860/12067 [04:59<32:08,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 20, max length 20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1900/12067 [05:05<34:04,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 1900 - SAVED!\n",
      "758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1930/12067 [05:10<21:00,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 19, max length 19.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2001/12067 [05:21<23:35,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 2000 - SAVED!\n",
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2047/12067 [05:29<25:23,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 17, max length 17.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2066/12067 [05:32<31:16,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 14, max length 14.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2100/12067 [05:38<23:41,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 2100 - SAVED!\n",
      "845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2102/12067 [05:39<25:20,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 26, max length 26.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2122/12067 [05:42<24:31,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 16, max length 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2200/12067 [05:54<25:21,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 2200 - SAVED!\n",
      "876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2213/12067 [05:57<34:21,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 24, max length 24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 2301/12067 [06:11<20:58,  7.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 2300 - SAVED!\n",
      "904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 2355/12067 [06:20<27:16,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 26, max length 26.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 2376/12067 [06:23<30:07,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 17, max length 17.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 2401/12067 [06:27<25:01,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 2400 - SAVED!\n",
      "948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2452/12067 [06:36<32:18,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 19, max length 19.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2501/12067 [06:44<26:13,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 2500 - SAVED!\n",
      "986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2509/12067 [06:45<21:57,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 32, max length 32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2533/12067 [06:48<18:53,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 17, max length 17.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2601/12067 [06:59<26:00,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 2600 - SAVED!\n",
      "1021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2603/12067 [06:59<27:34,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 21, max length 21.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2663/12067 [07:09<24:36,  6.37it/s]"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "perturbations = [\n",
    "    \"predicate\",\n",
    "    \"smart\",\n",
    "    \"s_o\", \n",
    "    \"bt\", \n",
    "    \"prn\", \n",
    "    \"dat\", \n",
    "    \"num\", \n",
    "    \"ent\", \n",
    "    \"neg\"\n",
    "]\n",
    "\n",
    "corrupt_percent = 0.8\n",
    "np.random.seed(2021)\n",
    "corrupt_indicies = np.random.choice(range(len(xsum_corrupted_train)), size=int(corrupt_percent*len(xsum_corrupted_train)))\n",
    "corruption_groups = np.array_split(corrupt_indicies, len(perturbations))\n",
    "\n",
    "total_corruption_count = 0\n",
    "iteration = 0\n",
    "\n",
    "#   client = CoreNLPClient(timeout=150000000, be_quiet=True, annotators=['openie'], \n",
    "#   endpoint='http://localhost:9002')\n",
    "#   client.start()\n",
    "os.makedirs(f'data/corruption_dict_{corrupt_percent}/', exist_ok=True)\n",
    "for pert_type, indicies in zip(perturbations, corruption_groups):\n",
    "    if pert_type == 'smart':\n",
    "        continue\n",
    "    corrupted_summaries = {}\n",
    "    already_done_tracker = []\n",
    "    total_corruption_count = 0\n",
    "    iteration = 0\n",
    "    if os.path.exists(f'data/corruption_dict_{corrupt_percent}/{pert_type}.pkl'):\n",
    "        with open(f'data/corruption_dict_{corrupt_percent}/{pert_type}.pkl', 'rb') as f:\n",
    "            corrupted_summaries = pickle.load(f)\n",
    "            already_done_tracker = corrupted_summaries.keys()\n",
    "            \n",
    "    print('Corruption: ' + pert_type)\n",
    "    \n",
    "    for index in tqdm.tqdm(indicies):\n",
    "\n",
    "        doc = xsum_corrupted_train[int(index)]\n",
    "        docid = doc['id']\n",
    "        \n",
    "        if docid in already_done_tracker:\n",
    "            continue\n",
    "            \n",
    "        summary = spacy(doc['summary'])\n",
    "        article = spacy(doc['document'])\n",
    "        \n",
    "        if pert_type == \"s_o\":\n",
    "            try:\n",
    "                new_summary, ags = s_o_swap(summary)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                new_summary = None\n",
    "                \n",
    "        elif pert_type == \"predicate\":\n",
    "                new_summary, ags = predicateswap(summary, article)\n",
    "        elif pert_type == \"smart\":\n",
    "            new_summary = None\n",
    "#             try:\n",
    "#                 new_summary, ags = smartswap(summary)\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "#                 new_summary = None\n",
    "        elif pert_type == \"bt\":\n",
    "            try:\n",
    "                new_summary, ags = backtranslate(summary)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                new_summary = None\n",
    "                \n",
    "        elif pert_type == \"prn\":\n",
    "            new_summary, ags = pronounswap(summary)\n",
    "        elif pert_type == \"dat\":\n",
    "            new_summary, ags = dateswap(summary, article)\n",
    "        elif pert_type == \"num\":\n",
    "            new_summary, ags = numberswap(summary, article)\n",
    "        elif pert_type == \"ent\":\n",
    "            new_summary, ags = entityswap(summary, article)\n",
    "        elif pert_type == \"neg\":\n",
    "            new_summary, ags = negation(summary)\n",
    "#         if not new_summary and not ags:\n",
    "#             new_summary = summary\n",
    "#             ags = []\n",
    "    #     except:\n",
    "    #         new_summary = None\n",
    "        if new_summary:\n",
    "            new_summary = addnoise(new_summary, ags)\n",
    "\n",
    "        if new_summary:\n",
    "            if new_summary.text != summary.text:\n",
    "              corrupted_summaries[docid] = new_summary.text\n",
    "              total_corruption_count += 1\n",
    "                \n",
    "        iteration +=1\n",
    "        if iteration % 100 == 0:\n",
    "          print(f\"{pert_type}, save_index: {iteration} - SAVED!\")\n",
    "          print(f\"{len(corrupted_summaries)}\")\n",
    "          with open(f'data/corruption_dict_{corrupt_percent}/{pert_type}.pkl', 'wb') as f:\n",
    "            pickle.dump(corrupted_summaries, f)\n",
    "    \n",
    "    \n",
    "    print('Corruption done and saved to: ' + 'data/corruption_dict/'+pert_type+'.pkl')\n",
    "    print('Total corruption_count = ' + str(total_corruption_count))\n",
    "    \n",
    "# client.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "perturbations = [\n",
    "    \"predicate\",\n",
    "    \"smart\",\n",
    "    \"s_o\", \n",
    "    \"bt\", \n",
    "    \"prn\", \n",
    "    \"dat\", \n",
    "    \"num\", \n",
    "    \"ent\", \n",
    "    \"neg\"\n",
    "]\n",
    "\n",
    "corrupt_percent = 0.8\n",
    "np.random.seed(2021)\n",
    "corrupt_indicies = np.random.choice(range(len(xsum_corrupted_train)), size=int(corrupt_percent*len(xsum_corrupted_train)))\n",
    "corruption_groups = np.array_split(corrupt_indicies, len(perturbations))\n",
    "\n",
    "total_corruption_count = 0\n",
    "iteration = 0\n",
    "\n",
    "#   client = CoreNLPClient(timeout=150000000, be_quiet=True, annotators=['openie'], \n",
    "#   endpoint='http://localhost:9002')\n",
    "#   client.start()\n",
    "os.makedirs(f'data/corruption_dict_{corrupt_percent}/', exist_ok=True)\n",
    "for pert_type, indicies in zip(perturbations, corruption_groups):\n",
    "    corrupted_summaries = {}\n",
    "    already_done_tracker = []\n",
    "    total_corruption_count = 0\n",
    "    iteration = 0\n",
    "    if os.path.exists(f'data/corruption_dict_{corrupt_percent}/{pert_type}.pkl'):\n",
    "        with open(f'data/corruption_dict_{corrupt_percent}/{pert_type}.pkl', 'rb') as f:\n",
    "            already_done_tracker = pickle.load(f).keys()\n",
    "            \n",
    "    print('Corruption: ' + pert_type)\n",
    "    \n",
    "    for index in tqdm.tqdm(indicies):\n",
    "\n",
    "        doc = xsum_corrupted_train[int(index)]\n",
    "        docid = doc['id']\n",
    "        \n",
    "        if docid in already_done_tracker:\n",
    "            continue\n",
    "            \n",
    "        summary = spacy(doc['summary'])\n",
    "        article = spacy(doc['document'])\n",
    "        \n",
    "        if pert_type == \"s_o\":\n",
    "            try:\n",
    "                new_summary, ags = s_o_swap(summary)\n",
    "            except:\n",
    "                print(e)\n",
    "                new_summary = None\n",
    "                \n",
    "        elif pert_type == \"predicate\":\n",
    "                new_summary, ags = predicateswap(summary, article)\n",
    "        elif pert_type == \"smart\":\n",
    "            try:\n",
    "                new_summary, ags = smartswap(summary)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                new_summary = None\n",
    "        elif pert_type == \"bt\":\n",
    "            try:\n",
    "                new_summary, ags = backtranslate(summary)\n",
    "            except:\n",
    "                print(e)\n",
    "                new_summary = None\n",
    "                \n",
    "        elif pert_type == \"prn\":\n",
    "            new_summary, ags = pronounswap(summary)\n",
    "        elif pert_type == \"dat\":\n",
    "            new_summary, ags = dateswap(summary, article)\n",
    "        elif pert_type == \"num\":\n",
    "            new_summary, ags = numberswap(summary, article)\n",
    "        elif pert_type == \"ent\":\n",
    "            new_summary, ags = entityswap(summary, article)\n",
    "        elif pert_type == \"neg\":\n",
    "            new_summary, ags = negation(summary)\n",
    "#         if not new_summary and not ags:\n",
    "#             new_summary = summary\n",
    "#             ags = []\n",
    "    #     except:\n",
    "    #         new_summary = None\n",
    "        if new_summary:\n",
    "            new_summary = addnoise(new_summary, ags)\n",
    "\n",
    "        if new_summary:\n",
    "            if new_summary.text != summary.text:\n",
    "              corrupted_summaries[docid] = new_summary.text\n",
    "              total_corruption_count += 1\n",
    "                \n",
    "        iteration +=1\n",
    "        \n",
    "        if iteration % 100 == 0:\n",
    "          print(f\"{pert_type}, save_index: {iteration} - SAVED!\")\n",
    "          print(f\"{len(corrupted_summaries)}\")\n",
    "          with open(f'data/corruption_dict_{corrupt_percent}/{pert_type}.pkl', 'wb') as f:\n",
    "            pickle.dump(corrupted_summaries, f)\n",
    "    \n",
    "    \n",
    "    print('Corruption done and saved to: ' + 'data/corruption_dict/'+pert_type+'.pkl')\n",
    "    print('Total corruption_count = ' + total_corruption_count)\n",
    "    \n",
    "# client.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119713"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_corrupted_train = xsum_corrupted_train.add_column('corrupted_summary', corrupted_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_corrupted_train.save_to_disk(\"data/xsum_new_corrupted/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backtranslate(summary):\n",
    "  new_lang = random.choice(ACCEPTED_LANGS)\n",
    "  summary_trans = translator.translate(summary, dest=new_lang)\n",
    "  summary_btrans = translator.translate(summary_trans.text, dest=SOURCE_LANG)\n",
    "\n",
    "  new_summary = summary_btrans\n",
    "  augmentation_span = (new_summary[0].i, new_summary[-1].i)\n",
    "\n",
    "  if summary == new_summary:\n",
    "    return None, None\n",
    "  else:\n",
    "    return new_summary, [augmentation_span]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_filtered_train = xsum_filtered_train.filter(lambda x: x['keep']).remove_columns('keep')\n",
    "xsum_filtered_val= xsum_filtered_train.filter(lambda x: x['keep']).remove_columns('keep')\n",
    "xsum_filtered_test= xsum_filtered_train.filter(lambda x: x['keep']).remove_columns('keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_corrupted_train = xsum_filtered_train\n",
    "corrupted_summaries = xsum_train_corrupted['summary']\n",
    "\n",
    "corrupt_percent = 0.4\n",
    "\n",
    "np.random.seed(2021)\n",
    "corrupt_indicies = np.random.choice(range(len(xsum_train_corrupted)), size=int(corrupt_percent*len(xsum_train_corrupted)))\n",
    "corruption_groups = np.split(corrupt_indicies, len(perturbations))\n",
    "\n",
    "total_corruption_count = 0\n",
    "for pert_type, indicies in zip(perturbations, corruption_groups):\n",
    "  for index in indicies:\n",
    "    if pert_type == \"bt\":\n",
    "      new_summary, ags = backtranslate(summary)\n",
    "    if pert_type == \"prn\":\n",
    "      new_summary, ags = pronounswap(summary)\n",
    "    if pert_type == \"dat\":\n",
    "      new_summary, ags = dateswap(summary, article)\n",
    "    if pert_type == \"num\":\n",
    "      new_summary, ags = numberswap(summary, article)\n",
    "    if pert_type == \"ent\":\n",
    "      new_summary, ags = entityswap(summary, article)\n",
    "    if pert_type == \"neg\":\n",
    "      new_summary, ags = negation(summary)\n",
    "    if not new_summary and not ags:\n",
    "      new_summary = summary\n",
    "      ags = []\n",
    "    new_summary = addnoise(new_summary, ags)\n",
    "    if new_summary:\n",
    "      corrupted_summaries[index] = new_summary.text\n",
    "      total_corruption_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
