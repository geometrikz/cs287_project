{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade\n",
    "# !pip install sagemaker transformers \n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::448807757624:role/service-role/AmazonSageMaker-ExecutionRole-20211202T101582\n",
      "sagemaker bucket: sagemaker-us-east-2-448807757624\n",
      "sagemaker session region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "s3_prefix = 'datasets/xsum_corrupted_entityonly_untokenized'\n",
    "\n",
    "xsum_corrupted_train = load_from_disk(\"data/xsum_corrupted_entityonly/train\")\n",
    "# xsum_corrupted_val= load_dataset(\"data/xsum_corrupted/val\")\n",
    "# xsum_corrupted_test= load_dataset(\"data/xsum_corrupted/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download Data: this takes a while for some reason.\n",
    "\"\"\"\n",
    "column_names = xsum_corrupted_train.column_names\n",
    "text_column, corrupted_column, target_column = \"document\", \"corrupted_summary\", \"summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef36cb260ff2437b81354e7c3dd54dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Process Data\n",
    "\"\"\"\n",
    "\n",
    "MODEL_NAME = 'facebook/bart-large'\n",
    "max_source_length = 1024\n",
    "max_target_length = 128\n",
    "padding=False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "MODEL_NAME, cache_dir=None, use_fast=True, revision=\"main\", use_auth_token=False,\n",
    ")\n",
    "\n",
    "sep_token = '</s>'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "\n",
    "    inputs = examples[text_column]\n",
    "    corruptions = examples[corrupted_column]\n",
    "    targets = examples[target_column]\n",
    "    \n",
    "    # Tokenize Input\n",
    "    model_inputs = tokenizer(\n",
    "        text = inputs, text_pair=corruptions, max_length=max_source_length, \n",
    "        padding=padding, truncation='only_first', add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, max_length=max_target_length, padding=padding, truncation=True\n",
    "        )\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "            for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = xsum_corrupted_train\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=None,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "# val_dataset = xsum_filtered_val\n",
    "# val_dataset = val_dataset.map(\n",
    "#     preprocess_function,\n",
    "#     batched=True,\n",
    "#     num_proc=None,\n",
    "#     remove_columns=column_names,\n",
    "#     load_from_cache_file=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# # save test_dataset to s3\n",
    "# test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "# val_dataset.save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
