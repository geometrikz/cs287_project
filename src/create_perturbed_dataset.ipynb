{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "import tqdm\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "spacy = nlp\n",
    "\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_filtered_train = load_from_disk(\"data/xsum_filtered/train\")\n",
    "# xsum_filtered_val = load_from_disk(\"data/xsum_filtered/val\")\n",
    "# xsum_filtered_test = load_from_disk(\"data/xsum_filtered/test\")\n",
    "xsum_filtered_train._data = xsum_filtered_train._data.filter(xsum_filtered_train['keep'])\n",
    "xsum_corrupted_train = xsum_filtered_train.add_column('corrupted_summary', xsum_filtered_train['summary'])\n",
    "xsum_corrupted_train = xsum_corrupted_train.add_column('corrupted_flag', ['False']* len(xsum_corrupted_train))\n",
    "\n",
    "# xsum_filtered_val._data = xsum_filtered_val._data.filter(xsum_filtered_val['keep'])\n",
    "# xsum_corrupted_val = xsum_filtered_val.add_column('corrupted_summary', xsum_filtered_val['summary'])\n",
    "\n",
    "# xsum_filtered_test._data = xsum_filtered_test._data.filter(xsum_filtered_test['keep'])\n",
    "# xsum_corrupted_test = xsum_filtered_test.add_column('corrupted_summary', xsum_filtered_test['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise function\n",
    "\n",
    "NOISE_PROB = 1\n",
    "DELETE_PROB = 0.8\n",
    "def addnoise(summary, augmentation_span):\n",
    "        summary_tokens = [token.text_with_ws for token in summary]\n",
    "\n",
    "        new_summary = []\n",
    "        for ix, token in enumerate(summary_tokens):\n",
    "            # don't modify text inside an augmented span\n",
    "            apply_augmentation = True\n",
    "            if augmentation_span:\n",
    "              for aug_span in augmentation_span:\n",
    "                if aug_span:\n",
    "                  span_start, span_end = aug_span\n",
    "                  if span_start <= ix <= span_end:\n",
    "                      apply_augmentation = False\n",
    "\n",
    "            # decide whether to add noise\n",
    "            if apply_augmentation and random.random() < NOISE_PROB:\n",
    "                # decide whether to replicate or delete token\n",
    "                if random.random() < DELETE_PROB:\n",
    "                    # update spans and skip token\n",
    "                    if augmentation_span:\n",
    "                      for el in range(0, len(augmentation_span)):\n",
    "                        aug_span = augmentation_span[el]\n",
    "                        if aug_span:\n",
    "                          span_start, span_end = aug_span\n",
    "                          if ix < span_start:\n",
    "                            span_start -= 1\n",
    "                            span_end -= 1\n",
    "                          aug_span = span_start, span_end\n",
    "                          augmentation_span[el] = aug_span\n",
    "                      if len(new_summary) > 0:\n",
    "                        if new_summary[-1][-1] != \" \":\n",
    "                          new_summary[-1] = new_summary[-1] + \" \"\n",
    "                      continue      \n",
    "                else:\n",
    "                  if augmentation_span:\n",
    "                    for el in range(0, len(augmentation_span)):\n",
    "                      aug_span = augmentation_span[el]\n",
    "                      if aug_span:\n",
    "                        span_start, span_end = aug_span\n",
    "                        if ix < span_start:\n",
    "                          span_start += 1\n",
    "                          span_end += 1\n",
    "                        aug_span = span_start, span_end\n",
    "                        augmentation_span[el] = aug_span\n",
    "\n",
    "                  new_summary.append(token)\n",
    "            new_summary.append(token)\n",
    "        new_summary = spacy(\"\".join(new_summary))\n",
    "\n",
    "        if summary.text == new_summary.text:\n",
    "#             return new_summary\n",
    "            return new_summary\n",
    "        else:\n",
    "            return new_summary\n",
    "        \n",
    "docs_to_noise = xsum_corrupted_train['id']\n",
    "docs_to_noise = np.random.choice(docs_to_noise, int(0.05*len(docs_to_noise)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_summary(example):\n",
    "    if example['id'] in docs_to_noise:\n",
    "        doc = nlp(example['summary'])\n",
    "        new_doc = addnoise(doc, [(0, len(doc))])\n",
    "        example['corrupted_summary'] = new_doc.text\n",
    "        return example\n",
    "    else:\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9dcf483eb8489ab97ba1eff3e373b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xsum_corrupted_train = xsum_corrupted_train.map(noise_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment our perturbations you don't want.\n",
    "\n",
    "perturbations = [\n",
    "    \"predicate\",\n",
    "    \"smart\",\n",
    "    \"s_o\", \n",
    "    \"bt\", \n",
    "#     \"prn\", \n",
    "#     \"dat\", \n",
    "#     \"num\", \n",
    "#     \"ent\", \n",
    "#     \"neg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a7d7dc075e464993e45590821fc83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 8536 predicate corruptions to the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b821edc17d4067bef66839a002c748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6641 smart corruptions to the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193999d4097e46289f9ad73e494f2559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 7078 s_o corruptions to the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67db1dfc71a4777ba1be5092104b545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10000 bt corruptions to the dataset\n"
     ]
    }
   ],
   "source": [
    "# Change num_each_perturbation if you want to add more perturbations to train on.\n",
    "num_each_perturbation = 10000\n",
    "for pert_type in perturbations:\n",
    "    \n",
    "    with open(f'data/corruption_dict/{pert_type}.pkl', 'rb') as f:\n",
    "        corrupted_summaries = pickle.load(f)\n",
    "        \n",
    "    def corrupt_summary(example):\n",
    "        docid = example['id']\n",
    "        if docid in corrupted_summaries:\n",
    "            example['corrupted_summary'] = corrupted_summaries[docid]\n",
    "            example['corrupted_flag'] = pert_type\n",
    "            return example\n",
    "        else:\n",
    "            return example\n",
    "        \n",
    "    filters = list(corrupted_summaries.keys())\n",
    "    \n",
    "    \n",
    "    filters = filters[:num_each_perturbation]\n",
    "    \n",
    "    corrupted_summaries = {ids: corrupted_summaries[ids] for ids in filters}\n",
    "    xsum_corrupted_train = xsum_corrupted_train.map(corrupt_summary)\n",
    "#     xsum_corrupted_train = xsum_corrupted_train.map(corrupt_summary)\n",
    "#     xsum_corrupted_train = xsum_corrupted_train.map(corrupt_summary)\n",
    "\n",
    "    print(f'Added {len(corrupted_summaries)} {pert_type} corruptions to the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_corrupted_train.save_to_disk(\"data/xsum_corrupted_predicateonly/train\")\n",
    "# xsum_corrupted_val.save_to_disk(\"data/xsum_corrupted/val\")\n",
    "# xsum_corrupted_test.save_to_disk(\"data/xsum_corrupted/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity only perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_filtered_train = load_from_disk(\"data/xsum_filtered/train\")\n",
    "# xsum_filtered_val = load_from_disk(\"data/xsum_filtered/val\")\n",
    "# xsum_filtered_test = load_from_disk(\"data/xsum_filtered/test\")\n",
    "xsum_filtered_train._data = xsum_filtered_train._data.filter(xsum_filtered_train['keep'])\n",
    "xsum_corrupted_train = xsum_filtered_train.add_column('corrupted_summary', xsum_filtered_train['summary'])\n",
    "xsum_corrupted_train = xsum_corrupted_train.add_column('corrupted_flag', ['False']* len(xsum_corrupted_train))\n",
    "\n",
    "# xsum_filtered_val._data = xsum_filtered_val._data.filter(xsum_filtered_val['keep'])\n",
    "# xsum_corrupted_val = xsum_filtered_val.add_column('corrupted_summary', xsum_filtered_val['summary'])\n",
    "\n",
    "# xsum_filtered_test._data = xsum_filtered_test._data.filter(xsum_filtered_test['keep'])\n",
    "# xsum_corrupted_test = xsum_filtered_test.add_column('corrupted_summary', xsum_filtered_test['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment our perturbations you don't want.\n",
    "\n",
    "perturbations = [\n",
    "#     \"predicate\",\n",
    "#     \"smart\",\n",
    "#     \"s_o\", \n",
    "    \"bt\", \n",
    "    \"prn\", \n",
    "    \"dat\", \n",
    "    \"num\", \n",
    "    \"ent\", \n",
    "#     \"neg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38577daea2e416ab7a0dc57cadec68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 8000 bt corruptions to the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef188f479f944258ae29a7676330eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3313 prn corruptions to the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c633770142d24989a99240a39c17ff7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2446 dat corruptions to the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6b99fee8dc4d70aa7043abeab65c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1967 num corruptions to the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3fcd420df548ce8559df52b49c03c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6505 ent corruptions to the dataset\n"
     ]
    }
   ],
   "source": [
    "# Change num_each_perturbation if you want to add more perturbations to train on.\n",
    "num_each_perturbation = 8000\n",
    "for pert_type in perturbations:\n",
    "    \n",
    "    with open(f'data/corruption_dict/{pert_type}.pkl', 'rb') as f:\n",
    "        corrupted_summaries = pickle.load(f)\n",
    "        \n",
    "    def corrupt_summary(example):\n",
    "        docid = example['id']\n",
    "        if docid in corrupted_summaries:\n",
    "            example['corrupted_summary'] = corrupted_summaries[docid]\n",
    "            example['corrupted_flag'] = pert_type\n",
    "            return example\n",
    "        else:\n",
    "            return example\n",
    "        \n",
    "    filters = list(corrupted_summaries.keys())\n",
    "    \n",
    "    \n",
    "    filters = filters[:num_each_perturbation]\n",
    "    \n",
    "    corrupted_summaries = {ids: corrupted_summaries[ids] for ids in filters}\n",
    "    xsum_corrupted_train = xsum_corrupted_train.map(corrupt_summary)\n",
    "#     xsum_corrupted_train = xsum_corrupted_train.map(corrupt_summary)\n",
    "#     xsum_corrupted_train = xsum_corrupted_train.map(corrupt_summary)\n",
    "\n",
    "    print(f'Added {len(corrupted_summaries)} {pert_type} corruptions to the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_corrupted_train.save_to_disk(\"data/xsum_corrupted_entityonly/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = list(corrupted_summaries.keys())[50:60]\n",
    "for test in test_cases:\n",
    "    ind = xsum_corrupted_train['id'].index(test)\n",
    "    print(\"Original:\\t\" + xsum_corrupted_train[ind]['summary'])\n",
    "    print(\"Corrupted:\\t\" + xsum_corrupted_train[ind]['corrupted_summary'])"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
