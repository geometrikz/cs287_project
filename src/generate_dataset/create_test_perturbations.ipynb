{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 01:13:04 WARNING: Directory ./corenlp already exists. Please install CoreNLP to a new directory.\n",
      "[nltk_data] Downloading package wordnet to /home/geoff/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers sentence-transformers stanza datasets rouge_score \n",
    "# !pip install spacy\n",
    "# ! python -m spacy download en_core_web_lg\n",
    "# !pip install googletrans==4.0.0-rc1\n",
    "# !pip install pyinflect\n",
    "# !pip install stanza\n",
    "\n",
    "import stanza\n",
    "\n",
    "corenlp_dir = './corenlp'\n",
    "stanza.install_corenlp(dir=corenlp_dir)\n",
    "\n",
    "# Set the CORENLP_HOME environment variable to point to the installation location\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pyinflect\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "# !pip install nltk --upgrade\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_ws(old_token, new_token):\n",
    "    # Align trailing whitespaces between tokens\n",
    "    if old_token[-1] == new_token[-1] == \" \":\n",
    "        return new_token\n",
    "    elif old_token[-1] == \" \":\n",
    "        return new_token + \" \"\n",
    "    elif new_token[-1] == \" \":\n",
    "        return new_token[:-1]\n",
    "    else:\n",
    "        return new_token\n",
    "    \n",
    "NEGATABLE_TOKENS = (\"are\", \"is\", \"was\", \"were\", \"have\", \"has\", \"had\",\n",
    "                                   \"do\", \"does\", \"did\", \"can\", \"ca\", \"could\", \"may\",\n",
    "                                   \"might\", \"must\", \"shall\", \"should\", \"will\", \"would\")\n",
    "\n",
    "\n",
    "\n",
    "def negation(summary):\n",
    "  candidate_tokens = [token for token in summary if token.text in NEGATABLE_TOKENS]\n",
    "\n",
    "  if not candidate_tokens:\n",
    "      return None, None\n",
    "\n",
    "  # choose random token to negate\n",
    "  negated_token = random.choice(candidate_tokens)\n",
    "  negated_index = negated_token.i\n",
    "  L = len(summary)\n",
    "\n",
    "  # negation occurs at the first negatable token (e.g. does not have)\n",
    "  if negated_index > 0:\n",
    "    if summary[negated_index-1].text in NEGATABLE_TOKENS:\n",
    "      negated_token = summary[negated_index-1]\n",
    "      negated_index = negated_index-1\n",
    "  \n",
    "  #check whether qualified by a negative\n",
    "  is_negative = False\n",
    "  if (L-1) > negated_index:\n",
    "    if summary[negated_index + 1].text in [\"not\", \"n't\"]:\n",
    "      is_negative = True\n",
    "    elif summary[negated_index+1].text == \"no\":\n",
    "      return None, None\n",
    "\n",
    "  #add not/n't if is_negative is False, remove if True\n",
    "  tokens = [token.text_with_ws for token in summary]\n",
    "  if is_negative:\n",
    "      if summary[negated_index + 1].text.lower() == \"n't\":\n",
    "          if summary[negated_index + 1].text.lower() == \"ca\":\n",
    "              tokens[negated_index] = \"can\" if tokens[negated_index].islower() else \"Can\"\n",
    "          tokens[negated_index] = tokens[negated_index] + \" \"\n",
    "      tokens.pop(negated_index + 1)\n",
    "\n",
    "  else:\n",
    "      if summary[negated_index].text.lower() in [\"am\", \"may\", \"might\", \"must\", \"shall\", \"will\"]:\n",
    "          negation = \"not \"\n",
    "      else:\n",
    "          negation = random.choice([\"not \", \"n't \"])\n",
    "\n",
    "      if negation == \"n't \":\n",
    "          if summary[negated_index].text.lower() == \"can\":\n",
    "              tokens[negated_index] = \"ca\" if tokens[negated_index].islower() else \"Ca\"\n",
    "          else:\n",
    "              tokens[negated_index] = tokens[negated_index][:-1]\n",
    "      tokens.insert(negated_index + 1, negation)\n",
    "  \n",
    "  new_summary = spacy(\"\".join(tokens))\n",
    "  augmentation_span = [(negated_index, negated_index if is_negative else negated_index + 1)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "CLASS_TO_PRONOUN = {\n",
    "            \"SUBJECT\": [\"you\", \"he\", \"she\", \"we\", \"they\"],\n",
    "            \"OBJECT\": [\"me\", \"you\", \"him\", \"her\", \"us\", \"them\"],\n",
    "            \"POSSESSIVE\": [\"my\", \"your\", \"his\", \"her\", \"its\", \"our\", \"your\", \"their\"],\n",
    "            \"REFLEXIVE\": [\"myself\", \"yourself\", \"himself\", \"itself\", \"ourselves\", \"yourselves\", \"themselves\"]\n",
    "        }\n",
    "\n",
    "\n",
    "PRONOUN_TO_CLASS = {pronoun: key for (key, values) in CLASS_TO_PRONOUN.items() for pronoun in values}\n",
    "PRONOUNS = [pronoun for pronoun in PRONOUN_TO_CLASS.keys()]\n",
    "\n",
    "\n",
    "def pronounswap(summary):\n",
    "  summary_pronouns = [token for token in summary if token.text.lower() in PRONOUNS]\n",
    "\n",
    "  if not summary_pronouns:\n",
    "    return None, None\n",
    "\n",
    "  swap = random.choice(summary_pronouns)\n",
    "  swap_index = swap.i\n",
    "  swap_class = PRONOUN_TO_CLASS[swap.text.lower()]\n",
    "\n",
    "  candidate_tokens = [token for token in CLASS_TO_PRONOUN[swap_class] if token != swap.text.lower()]\n",
    "\n",
    "  if not candidate_tokens:\n",
    "    return None, None\n",
    "\n",
    "  swapped_token = random.choice(candidate_tokens)\n",
    "  swapped_token = align_ws(swap.text_with_ws, swapped_token)\n",
    "  swapped_token = swapped_token if swap.text.islower() else swapped_token.capitalize()\n",
    "\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  summary_tokens[swap_index] = swapped_token\n",
    "\n",
    "  new_summary = spacy(\"\".join(summary_tokens))\n",
    "\n",
    "  augmentation_span = [(swap_index, swap_index)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "\n",
    "ENTITY_CATEGORIES = (\"PERSON\", \"ORG\", \"NORP\", \"FAC\", \"GPE\", \"LOC\", \"PRODUCT\",\n",
    "                           \"WORK_OF_ART\", \"EVENT\")\n",
    "\n",
    "def entityswap(summary, source):\n",
    "  source_ents = [ent for ent in source.ents if ent.label_ in ENTITY_CATEGORIES]\n",
    "  summary_ents = [ent for ent in summary.ents if ent.label_ in ENTITY_CATEGORIES]\n",
    "\n",
    "  if not source_ents or not summary_ents:\n",
    "    return None, None\n",
    "\n",
    "  swap = random.choice(summary_ents)\n",
    "  candidate_ents = [ent for ent in source_ents if ent.text != swap.text and ent.text not in swap.text and swap.text not in ent.text]\n",
    "\n",
    "  if not candidate_ents:\n",
    "    return None, None\n",
    "\n",
    "  swapped_ent = random.choice(candidate_ents)\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  swapped_token = align_ws(swap.text_with_ws, swapped_ent.text_with_ws)\n",
    "  summary_tokens = summary_tokens[:swap.start] + [swapped_token] + summary_tokens[swap.end:]\n",
    "\n",
    "  new_summary = spacy(\"\".join(summary_tokens))\n",
    "  augmentation_span = [(swap.start, swap.start + len(swapped_ent) - 1)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "NUMBER_CATEGORIES = (\"PERCENT\", \"MONEY\", \"QUANTITY\", \"CARDINAL\")\n",
    "\n",
    "def numberswap(summary, source):\n",
    "  source_ents = [ent for ent in source.ents if ent.label_ in NUMBER_CATEGORIES]\n",
    "  summary_ents = [ent for ent in summary.ents if ent.label_ in NUMBER_CATEGORIES]\n",
    "\n",
    "  if not source_ents or not summary_ents:\n",
    "    return None, None\n",
    "\n",
    "  swap = random.choice(summary_ents)\n",
    "  candidate_ents = [ent for ent in source_ents if ent.text != swap.text and ent.text not in swap.text and swap.text not in ent.text]\n",
    "\n",
    "  if not candidate_ents:\n",
    "    return None, None\n",
    "\n",
    "  swapped_ent = random.choice(candidate_ents)\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  swapped_token = align_ws(swap.text_with_ws, swapped_ent.text_with_ws)\n",
    "  summary_tokens = summary_tokens[:swap.start] + [swapped_token] + summary_tokens[swap.end:]\n",
    "\n",
    "  new_summary = spacy(\"\".join(summary_tokens))\n",
    "  augmentation_span = [(swap.start, swap.start + len(swapped_ent) - 1)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "DATE_CATEGORIES = (\"DATE\", \"TIME\")\n",
    "\n",
    "def dateswap(summary, source):\n",
    "  source_ents = [ent for ent in source.ents if ent.label_ in DATE_CATEGORIES]\n",
    "  summary_ents = [ent for ent in summary.ents if ent.label_ in DATE_CATEGORIES]\n",
    "\n",
    "  if not source_ents or not summary_ents:\n",
    "    return None, None\n",
    "\n",
    "  swap = random.choice(summary_ents)\n",
    "  candidate_ents = [ent for ent in source_ents if ent.text != swap.text and ent.text not in swap.text and swap.text not in ent.text]\n",
    "\n",
    "  if not candidate_ents:\n",
    "    return None, None\n",
    "\n",
    "  swapped_ent = random.choice(candidate_ents)\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  swapped_token = align_ws(swap.text_with_ws, swapped_ent.text_with_ws)\n",
    "  summary_tokens = summary_tokens[:swap.start] + [swapped_token] + summary_tokens[swap.end:]\n",
    "\n",
    "  new_summary = spacy(\"\".join(summary_tokens))\n",
    "  augmentation_span = [(swap.start, swap.start + len(swapped_ent) - 1)]\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "\n",
    "POS_TAGS = (\"NOUN\", \"ADJ\", \"PROPN\")\n",
    "\n",
    "def openIE_filter(extract, summary):\n",
    "  new_text = \"\"\n",
    "  extract = spacy(extract)\n",
    "  start_index = 0\n",
    "  for token in summary:\n",
    "    if token.text != extract[0].text:\n",
    "      start_index+=1\n",
    "    else:\n",
    "      break\n",
    "  tracking_index = start_index\n",
    "  for word in extract:\n",
    "    while (summary[tracking_index].text != word.text):\n",
    "      tracking_index+=1\n",
    "    if summary[tracking_index].pos_ in POS_TAGS:\n",
    "      break\n",
    "  start_index = tracking_index\n",
    "  while (summary[tracking_index].pos_ != \"NOUN\" and summary[tracking_index].pos_ != \"PROPN\" and tracking_index < len(summary)):\n",
    "    new_text+=(summary[tracking_index].text_with_ws)\n",
    "    tracking_index += 1\n",
    "  if len(summary) == tracking_index:\n",
    "    return new_text, start_index, tracking_index\n",
    "  while (summary[tracking_index].pos_ == \"NOUN\" or summary[tracking_index].pos_ == \"PROPN\"):\n",
    "    new_text+=(summary[tracking_index].text_with_ws)\n",
    "    tracking_index += 1\n",
    "  return new_text, start_index, tracking_index\n",
    "\n",
    "\n",
    "def s_o_swap(summary):\n",
    "  summary_ents = [ent for ent in summary.ents]\n",
    "  story_triples = []\n",
    "  document = client.annotate(summary.text, output_format='json')\n",
    "  triples = []\n",
    "  for sentence in document['sentences']:\n",
    "      for triple in sentence['openie']:\n",
    "          triples.append({\n",
    "            'subject': triple['subject'],\n",
    "            'relation': triple['relation'],\n",
    "              'object': triple['object']\n",
    "          })\n",
    "  if not triples:\n",
    "    return None, None\n",
    "  candidate_triples = []\n",
    "  for triple in triples:\n",
    "    subj = triple['subject']\n",
    "    obj = triple['object']\n",
    "    allow_triple = False\n",
    "    for ent in summary_ents:\n",
    "      if subj == ent.text or subj in ent.text or ent.text in subj:\n",
    "        allow_triple = True\n",
    "      if obj == ent.text or obj in ent.text or ent.text in obj:\n",
    "        allow_triple = True\n",
    "    if allow_triple == True:\n",
    "      candidate_triples.append(triple)\n",
    "  if not candidate_triples:\n",
    "    return None, None\n",
    "  triple_swap = random.choice(candidate_triples)\n",
    "  triple_swap[\"subject\"], s_start, s_end = openIE_filter(triple_swap[\"subject\"], summary)\n",
    "  triple_swap[\"object\"], o_start, o_end = openIE_filter(triple_swap[\"object\"], summary)\n",
    "  if s_start == 0:\n",
    "    triple_swap[\"object\"] = triple_swap[\"object\"].capitalize()\n",
    "    if summary[s_start].pos_ != \"PROPN\":\n",
    "      triple_swap[\"subject\"] = triple_swap[\"subject\"][0].lower() + triple_swap[\"subject\"][1:]\n",
    "  if o_start == 0:\n",
    "    triple_swap[\"subject\"] = triple_swap[\"subject\"].capitalize()\n",
    "    if summary[o_start].pos_ != \"PROPN\":\n",
    "      triple_swap[\"object\"] = triple_swap[\"object\"][0].lower() + triple_swap[\"object\"][1:]\n",
    "  if s_end <= o_start:\n",
    "    new_summary = summary[:s_start].text_with_ws + triple_swap[\"object\"] + summary[s_end:o_start].text_with_ws + triple_swap[\"subject\"] + summary[o_end:].text_with_ws\n",
    "    augmentation_span = [(s_start, o_end - o_start + s_start-1), (o_end - s_end + s_start, o_end-1)]\n",
    "  elif o_end <= s_start:\n",
    "    new_summary = summary[:o_start].text_with_ws + triple_swap[\"subject\"] + summary[o_end:s_start].text_with_ws + triple_swap[\"object\"] + summary[s_end:].text_with_ws\n",
    "    augmentation_span = [(o_start, s_end - s_start + o_start-1), (s_end - o_end + o_start-1, s_end-2)]\n",
    "  else:\n",
    "    return None, None\n",
    "  new_summary = spacy(new_summary)\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n",
    "\n",
    "NOISE_PROB = 0.05\n",
    "DELETE_PROB = 0.8\n",
    "def addnoise(summary, augmentation_span):\n",
    "        summary_tokens = [token.text_with_ws for token in summary]\n",
    "\n",
    "        new_summary = []\n",
    "        for ix, token in enumerate(summary_tokens):\n",
    "            # don't modify text inside an augmented span\n",
    "            apply_augmentation = True\n",
    "            if augmentation_span:\n",
    "              for aug_span in augmentation_span:\n",
    "                if aug_span:\n",
    "                  span_start, span_end = aug_span\n",
    "                  if span_start <= ix <= span_end:\n",
    "                      apply_augmentation = False\n",
    "\n",
    "            # decide whether to add noise\n",
    "            if apply_augmentation and random.random() < NOISE_PROB:\n",
    "                # decide whether to replicate or delete token\n",
    "                if random.random() < DELETE_PROB:\n",
    "                    # update spans and skip token\n",
    "                    if augmentation_span:\n",
    "                      for el in range(0, len(augmentation_span)):\n",
    "                        aug_span = augmentation_span[el]\n",
    "                        if aug_span:\n",
    "                          span_start, span_end = aug_span\n",
    "                          if ix < span_start:\n",
    "                            span_start -= 1\n",
    "                            span_end -= 1\n",
    "                          aug_span = span_start, span_end\n",
    "                          augmentation_span[el] = aug_span\n",
    "                      if len(new_summary) > 0:\n",
    "                        if new_summary[-1][-1] != \" \":\n",
    "                          new_summary[-1] = new_summary[-1] + \" \"\n",
    "                      continue      \n",
    "                else:\n",
    "                  if augmentation_span:\n",
    "                    for el in range(0, len(augmentation_span)):\n",
    "                      aug_span = augmentation_span[el]\n",
    "                      if aug_span:\n",
    "                        span_start, span_end = aug_span\n",
    "                        if ix < span_start:\n",
    "                          span_start += 1\n",
    "                          span_end += 1\n",
    "                        aug_span = span_start, span_end\n",
    "                        augmentation_span[el] = aug_span\n",
    "\n",
    "                  new_summary.append(token)\n",
    "            new_summary.append(token)\n",
    "        new_summary = spacy(\"\".join(new_summary))\n",
    "\n",
    "        if summary.text == new_summary.text:\n",
    "            return None\n",
    "        else:\n",
    "            return new_summary\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def tree_traverse(tree):\n",
    "  used_names = []\n",
    "  used_names.append(tree[0].name().split(\".\")[0])\n",
    "  while tree[1:]:\n",
    "    tree = random.choice(tree[1:])\n",
    "    used_names.append(tree[0].name().split(\".\")[0])\n",
    "  return used_names\n",
    "\n",
    "def find_syn(word):\n",
    "  syns = wn.synsets(word)\n",
    "  if not syns:\n",
    "    return None\n",
    "  syn = random.choice(syns)\n",
    "  return syn\n",
    "\n",
    "def pos_to_wn(pos):\n",
    "  if pos == \"VERB\":\n",
    "    return wn.VERB\n",
    "  if pos == \"ADV\":\n",
    "    return wn.ADV\n",
    "  if pos == \"ADJ\":\n",
    "    return wn.ADJ\n",
    "  \n",
    "\n",
    "\n",
    "def rand(a, b):\n",
    "  if not a:\n",
    "    return b\n",
    "  elif not b:\n",
    "    return a\n",
    "  else:\n",
    "    r = random.random()\n",
    "    if r < 0.5:\n",
    "      return a\n",
    "    else:\n",
    "      return b\n",
    "\n",
    "\n",
    "def tree_pert(word, POS):\n",
    "  synonyms = []\n",
    "  antonyms = []\n",
    "\n",
    "  ssets = wn.synsets(word, pos=pos_to_wn(POS))\n",
    "  if not ssets:\n",
    "    ssets = wn.synsets(word)\n",
    "    \n",
    "  for syn in ssets:\n",
    "      for l in syn.lemmas():\n",
    "          synonyms.append(l.name())\n",
    "          if l.antonyms():\n",
    "              antonyms.append(l.antonyms()[0].name())\n",
    "  if not synonyms and not antonyms:\n",
    "    return None\n",
    "  pert_base = rand(synonyms, antonyms)\n",
    "  pert_set = []\n",
    "  for el in pert_base:\n",
    "    if el != word:\n",
    "      pert_set.append(el)\n",
    "  for wrd in pert_base:\n",
    "    N = random.randint(1, 50)\n",
    "    for iter in range(0, N):\n",
    "      syn = find_syn(wrd)\n",
    "      if not syn:\n",
    "        break\n",
    "      tree = wn.synset(syn.name()).mst(lambda s:s.hyponyms())\n",
    "      choices = tree_traverse(tree)\n",
    "      wrd = random.choice(choices)\n",
    "      pert_set.append(wrd)\n",
    "\n",
    "  pert_set = list(set(pert_set))\n",
    "  new = random.choice(pert_set)\n",
    "  return new\n",
    "\n",
    "\n",
    "def generate_new(word, POS):\n",
    "  new = tree_pert(word, POS)\n",
    "  i = 0\n",
    "  while new == word and i < 20:\n",
    "    new = tree_pert(word, POS)\n",
    "    i+=1\n",
    "  if not new:\n",
    "    return None\n",
    "  new = new.split(\"_\")\n",
    "  new_word = \"\"\n",
    "  for i in range(0, len(new)):\n",
    "    if i < len(new) - 1:\n",
    "      new_word+=(new[i] + \" \")\n",
    "    else:\n",
    "      new_word+=new[i]\n",
    "  return new_word\n",
    "\n",
    "\n",
    "PERTURBABLE = (\"ADJ\", \"ADV\", \"VERB\")\n",
    "\n",
    "\n",
    "\n",
    "def smartswap(summary):  \n",
    "  document = client.annotate(summary.text, output_format='json')\n",
    "  triples = []\n",
    "  for sentence in document['sentences']:\n",
    "      for triple in sentence['openie']:\n",
    "          triples.append({\n",
    "            'subject': triple['subject'],\n",
    "            'relation': triple['relation'],\n",
    "              'object': triple['object']\n",
    "          })\n",
    "  if not triples:\n",
    "    return None, None\n",
    "  candidate_triples = triples\n",
    "  triple_pert = random.choice(candidate_triples)\n",
    "  triple_rel = spacy(triple_pert[\"relation\"] + \" \" + triple_pert[\"object\"])\n",
    "  triple_rel = [token.text for token in triple_rel]\n",
    "  target_tokens = [token for token in summary if token.text in triple_rel and token.pos_ in PERTURBABLE and token.text!=\"-\"]\n",
    "  timer = 0\n",
    "  while not target_tokens and timer < 20:\n",
    "    triple_pert = random.choice(candidate_triples)\n",
    "    triple_rel = spacy(triple_pert[\"relation\"] + \" \" + triple_pert[\"object\"])\n",
    "    triple_rel = [token.text for token in triple_rel]\n",
    "    target_tokens = [token for token in summary if token.text in triple_rel and token.pos_ in PERTURBABLE]\n",
    "    timer+=1\n",
    "\n",
    "  if not target_tokens:\n",
    "    return None, None\n",
    "  if len(target_tokens) > 4:\n",
    "    target_tokens = random.sample(target_tokens, 4)\n",
    "  \n",
    "  indices = [token.i for token in target_tokens]\n",
    "  replacements = []\n",
    "  span_len = []\n",
    "  for word in target_tokens:\n",
    "    new = generate_new(word.text, word.pos_)\n",
    "    if not new:\n",
    "      new = word.text\n",
    "    if new[0].isupper:\n",
    "      new = new[0].lower() + new[1:]\n",
    "    replacements.append(spacy(new))\n",
    "    span_len.append(new.count(\" \") + 1)\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  for ix in range(0, len(indices)):\n",
    "    summary_tokens[indices[ix]] = replacements[ix].text_with_ws + \" \"\n",
    "  new_summary = \"\".join(summary_tokens)\n",
    "\n",
    "  new_summary = spacy(new_summary)\n",
    "  augmentation_span = []\n",
    "  for ix in range(0, len(indices)):\n",
    "    augmentation_span.append((indices[ix], indices[ix] + span_len[ix] - 1))\n",
    "\n",
    "  new_summary_tokens = [token.text_with_ws for token in new_summary]\n",
    "\n",
    "  for ix in range(0, len(indices)):\n",
    "    if summary[indices[ix]].pos_ == \"VERB\":\n",
    "      nlp_sum = nlp(summary.text)\n",
    "      nlp_new = nlp(new_summary.text)\n",
    "      if new_summary[indices[ix]].pos_ == \"VERB\":\n",
    "        adj = nlp_new[indices[ix]]._.inflect(nlp_sum[indices[ix]].tag_)\n",
    "        if adj:\n",
    "          new_summary_tokens[indices[ix]] = adj + \" \"\n",
    "\n",
    "  new_summary = \"\".join(new_summary_tokens)\n",
    "\n",
    "  new_summary = spacy(new_summary)\n",
    "\n",
    "  \n",
    "  if summary.text == new_summary.text:\n",
    "      return None, None\n",
    "  else:\n",
    "      return new_summary, augmentation_span\n",
    "\n",
    "\n",
    "def predicateswap(summary):\n",
    "  summary_preds = [token for token in summary if token.pos_ == \"VERB\" or token.pos_ == \"ADV\" or token.pos_ == \"ADJ\" and token.text!=\"-\"]\n",
    "  if not summary_preds:\n",
    "      return None, None\n",
    "  original = random.choice(summary_preds)\n",
    "  ix = original.i\n",
    "  new = generate_new(original.text, original.pos_)\n",
    "  if not new:\n",
    "    return None, None\n",
    "  span_len = new.count(\" \") + 1\n",
    "\n",
    "  summary_tokens = [token.text_with_ws for token in summary]\n",
    "  summary_tokens[ix] = new\n",
    "  if ix < len(summary_tokens) - 1 and summary_tokens[ix+1] != \"-\":\n",
    "    summary_tokens[ix] += \" \"\n",
    "  new_summary = \"\".join(summary_tokens)\n",
    "  new_summary = spacy(new_summary)\n",
    "  augmentation_span = [(ix, ix + span_len - 1)]\n",
    "\n",
    "\n",
    "  new_summary_tokens = [token.text_with_ws for token in new_summary]\n",
    "  \n",
    "  if summary[ix].pos_ == \"VERB\":\n",
    "    nlp_sum = nlp(summary.text)\n",
    "    nlp_new = nlp(new_summary.text)\n",
    "    if new_summary[ix].pos_ == \"VERB\":\n",
    "      adj = nlp_new[ix]._.inflect(nlp_sum[ix].tag_)\n",
    "      if adj:\n",
    "        new_summary_tokens[ix] = adj + \" \"\n",
    "\n",
    "  new_summary = \"\".join(new_summary_tokens)\n",
    "\n",
    "  new_summary = spacy(new_summary)\n",
    "\n",
    "  if new_summary.text == summary.text:\n",
    "    return None, None\n",
    "  else:\n",
    "    return new_summary, augmentation_span\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk('data/xsum_filtered/val')\n",
    "dataset._data = dataset._data.filter(dataset['keep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #RUN THIS CELL TO START THE CORENLP TAGGER\n",
    "# client = CoreNLPClient(timeout=150000000, be_quiet=True, annotators=['openie'], \n",
    "# endpoint='http://localhost:9002')\n",
    "# client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt update\n",
    "# !apt install default-jre\n",
    "# conda install -c anaconda openjdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption: predicate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████                                                                 | 101/510 [00:13<00:45,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate, save_index: 100 - SAVED!\n",
      "96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████████████████████▉                                                 | 201/510 [00:25<00:33,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate, save_index: 200 - SAVED!\n",
      "190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████████████████▊                                 | 301/510 [00:37<00:22,  9.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate, save_index: 300 - SAVED!\n",
      "286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████████████████████████████████████████████████████████████▋                 | 401/510 [00:49<00:13,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate, save_index: 400 - SAVED!\n",
      "378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████████████████████████████████████▌ | 501/510 [01:02<00:00,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicate, save_index: 500 - SAVED!\n",
      "469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 510/510 [01:03<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption done and saved to: data/corruption_dict_test/test_predicate.pkl\n",
      "Total corruption_count = 491\n",
      "Corruption: smart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/509 [00:00<?, ?it/s]2021-12-11 01:16:40 INFO: Starting server with command: java -Xmx5G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 8889 -timeout 150000000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-93a781ec458c47ae.props -annotators openie -preload -outputFormat serialized\n",
      " 20%|████████████████                                                                 | 101/509 [00:26<01:07,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart, save_index: 100 - SAVED!\n",
      "81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████▏                                                | 202/509 [00:44<00:33,  9.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart, save_index: 200 - SAVED!\n",
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████████████████▉                                 | 301/509 [01:00<00:30,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart, save_index: 300 - SAVED!\n",
      "222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████████████████████████████████████████████████████████████▊                 | 401/509 [01:17<00:18,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart, save_index: 400 - SAVED!\n",
      "297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████████████████████████████████████▋ | 501/509 [01:33<00:01,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart, save_index: 500 - SAVED!\n",
      "367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 509/509 [01:34<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption done and saved to: data/corruption_dict_test/test_smart.pkl\n",
      "Total corruption_count = 385\n",
      "Corruption: s_o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▉                                                                                | 12/509 [00:01<00:47, 10.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 24, max length 24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▋                                                                            | 35/509 [00:04<01:04,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 38, max length 38.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████▊                                                                      | 73/509 [00:09<01:02,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 18, max length 18.\n",
      "[E040] Attempt to access token at 12, max length 12.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▉                                                                 | 100/509 [00:12<00:52,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 100 - SAVED!\n",
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████▋                                                             | 124/509 [00:15<00:51,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 12, max length 12.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████████████████████▉                                                 | 201/509 [00:25<00:35,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 200 - SAVED!\n",
      "119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████████████▌                                            | 230/509 [00:28<00:37,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 31, max length 31.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████████████████████████████                                 | 302/509 [00:36<00:21,  9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 300 - SAVED!\n",
      "183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████▍                          | 342/509 [00:41<00:19,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 47, max length 47.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████████████████████████████████████████████████████████████▊                 | 401/509 [00:49<00:14,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 400 - SAVED!\n",
      "250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████████████████████████████████████████████████████████████████████████▉   | 490/509 [00:59<00:02,  9.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E040] Attempt to access token at 30, max length 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████████████████████████████████████▋ | 501/509 [01:01<00:00,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_o, save_index: 500 - SAVED!\n",
      "311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 509/509 [01:02<00:00,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption done and saved to: data/corruption_dict_test/test_s_o.pkl\n",
      "Total corruption_count = 328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "perturbations = [\n",
    "    \"predicate\",\n",
    "    \"smart\",\n",
    "    \"s_o\",\n",
    "]\n",
    "\n",
    "corrupt_percent = 0.2\n",
    "np.random.seed(2021)\n",
    "corrupt_indicies = np.random.choice(range(len(dataset)), size=int(corrupt_percent*len(dataset)))\n",
    "corruption_groups = np.array_split(corrupt_indicies, len(perturbations))\n",
    "\n",
    "total_corruption_count = 0\n",
    "iteration = 0\n",
    "\n",
    "spacy = nlp\n",
    "\n",
    "output_dir = 'data/corruption_dict_test'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "no_noise = True\n",
    "\n",
    "for pert_type, indicies in zip(perturbations, corruption_groups):\n",
    "    corrupted_summaries = {}\n",
    "    already_done_tracker = []\n",
    "    total_corruption_count = 0\n",
    "    iteration = 0\n",
    "    if not no_noise:\n",
    "        if os.path.exists(output_dir + f'/test_{pert_type}.pkl'):\n",
    "            print('loaded previous file')\n",
    "            with open(output_dir + f'/test_{pert_type}.pkl', 'rb') as f:\n",
    "\n",
    "                corrupted_summaries = pickle.load(f)\n",
    "                already_done_tracker = corrupted_summaries.keys()\n",
    "\n",
    "    print('Corruption: ' + pert_type)\n",
    "    \n",
    "    for index in tqdm.tqdm(indicies):\n",
    "\n",
    "        doc = dataset[int(index)]\n",
    "        docid = doc['id']\n",
    "        \n",
    "        if docid in already_done_tracker:\n",
    "            continue\n",
    "            \n",
    "        summary = nlp(doc['summary'])\n",
    "        article = nlp(doc['document'])\n",
    "        \n",
    "        if pert_type == \"s_o\":\n",
    "            try:\n",
    "                new_summary, ags = s_o_swap(summary)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                new_summary = None\n",
    "                \n",
    "        elif pert_type == \"predicate\":\n",
    "                new_summary, ags = predicateswap(summary)\n",
    "        elif pert_type == \"smart\":\n",
    "            try:\n",
    "                new_summary, ags = smartswap(summary)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                new_summary = None\n",
    "\n",
    "        if new_summary and pert_type != 'bt' and not no_noise:\n",
    "#             print('nonoise')\n",
    "            new_summary = addnoise(new_summary, ags)\n",
    "\n",
    "        if new_summary:\n",
    "            if new_summary.text != summary.text:\n",
    "              corrupted_summaries[docid] = new_summary.text\n",
    "              total_corruption_count += 1\n",
    "                \n",
    "        iteration +=1\n",
    "        if iteration % 100 == 0:\n",
    "          print(f\"{pert_type}, save_index: {iteration} - SAVED!\")\n",
    "          print(f\"{len(corrupted_summaries)}\")\n",
    "          with open(output_dir +  f'/test_{pert_type}.pkl', 'wb') as f:\n",
    "            pickle.dump(corrupted_summaries, f)\n",
    "    \n",
    "    \n",
    "    print('Corruption done and saved to: ' + output_dir + f'/test_{pert_type}.pkl')\n",
    "    print('Total corruption_count = ' + str(total_corruption_count))\n",
    "    \n",
    "# client.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xsum_corrupted_train = xsum_corrupted_train.add_column('corrupted_summary', corrupted_summaries)\n",
    "# xsum_corrupted_train.save_to_disk(\"data/xsum_new_corrupted/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for docid in list(corrupted_summaries.keys())[:30]:\n",
    "    ind = xsum_corrupted_train['id'].index(docid)\n",
    "    print('original:\\t'+xsum_corrupted_train[ind]['summary'])\n",
    "    print('corrupt:\\t' + corrupted_summaries[docid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs282",
   "language": "python",
   "name": "cs282"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
