{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter XSUM DATASET\n",
    "# If first time, use this.\n",
    "# !python -m spacy download en_core_web_lg\n",
    "import spacy\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/amazon-research/fact-check-summarization\n",
    "\"\"\"\n",
    "\n",
    "def entity_match(ent, source, level=2):\n",
    "#     if level == 0:\n",
    "#         # case sensitive match\n",
    "#         if ent in source:\n",
    "#             return [ent,]\n",
    "#         else:\n",
    "#             return []\n",
    "#     elif level == 1:\n",
    "#         # case insensitive match\n",
    "#         if re.search(re.escape(ent), source, re.IGNORECASE):\n",
    "#             return [ent,]\n",
    "#         else:\n",
    "#             return []\n",
    "#     elif level == 2:\n",
    "#         split entity and match non-stop words\n",
    "    ent_split = ent.split()\n",
    "    result = []\n",
    "    for l in range(len(ent_split), 1, -1):\n",
    "        for start_i in range(len(ent_split) - l + 1):\n",
    "            sub_ent = \" \".join(ent_split[start_i:start_i+l])\n",
    "            if re.search(re.escape(sub_ent), source, re.IGNORECASE):\n",
    "                result.append(sub_ent)\n",
    "        if result:\n",
    "            break\n",
    "    if result:\n",
    "        return result\n",
    "    else:\n",
    "        for token in ent_split:\n",
    "            if token.lower() not in STOP_WORDS or token == \"US\":\n",
    "                if re.search(re.escape(token), source, re.IGNORECASE):\n",
    "                    result.append(token)\n",
    "        return result\n",
    "    return []\n",
    "\n",
    "\n",
    "\n",
    "def select_example(intro, abstract, filter_level=2):\n",
    "    entities_to_track = ['PERSON', 'FAC', 'GPE', 'ORG', 'NORP', 'LOC', 'EVENT']\n",
    "    doc = nlp(abstract)\n",
    "    en_count_in_summary = 0\n",
    "    select = True\n",
    "    for e in doc.ents:\n",
    "        if e[0].ent_type_ in entities_to_track:\n",
    "            en_count_in_summary += 1\n",
    "            match_result = entity_match(e.text, intro, 2)\n",
    "#             print(e, match_result)\n",
    "            if not match_result:\n",
    "                select = False\n",
    "                break\n",
    "    # if select and en_count_in_summary>0:\n",
    "    if select:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8e4fb03a59442685142de458f78c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe2a75d32b64121a43f987f139deddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/954 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset xsum/default (download: 245.38 MiB, generated: 507.60 MiB, post-processed: Unknown size, total: 752.98 MiB) to /root/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b150980dfd9d44b8a6d0e238f10251cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset xsum downloaded and prepared to /root/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb7b91540864e1ca515e63f821cf7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "xsum = load_dataset('xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "for i in range(5):\n",
    "    print(select_example(xsum['train']['document'][i], xsum['train']['summary'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8bc0dd08db43c3bd5b4c8dc302e24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204045 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cbc85c3bd14a2392c7c4e20e3360e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11332 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86de5fa77c5941bbb2c33317e84715b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11334 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def check_entity_agreement(example, select_example=select_example):\n",
    "    keep = select_example(example['document'], example['summary'])\n",
    "    example['keep'] = keep\n",
    "    return example\n",
    "# # This was the biggest pain to parallelize.....\n",
    "xsum_train_filtered = xsum['train'].map(check_entity_agreement)\n",
    "xsum_val_filtered = xsum['validation'].map(check_entity_agreement)\n",
    "xsum_test_filtered = xsum['test'].map(check_entity_agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "xsum_train_filtered.save_to_disk(\"data/xsum_filtered/train\")\n",
    "xsum_val_filtered.save_to_disk(\"data/xsum_filtered/val\")\n",
    "xsum_test_filtered.save_to_disk(\"data/xsum_filtered/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::448807757624:role/service-role/AmazonSageMaker-ExecutionRole-20211202T101582\n",
      "sagemaker bucket: sagemaker-us-east-2-448807757624\n",
      "sagemaker session region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "from datasets import load_from_disk\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "xsum_filtered_train = load_from_disk('data/xsum_filtered/train')\n",
    "xsum_filtered_train._data = xsum_filtered_train._data.filter(xsum_filtered_train['keep'])\n",
    "\n",
    "xsum_filtered_test = load_from_disk('data/xsum_filtered/test')\n",
    "xsum_filtered_test._data = xsum_filtered_test._data.filter(xsum_filtered_test['keep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please set do_save to True if you really want to save.\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "\n",
    "do_save = False\n",
    "\n",
    "if do_save:\n",
    "    s3_prefix = 'datasets/xsum_filtered'\n",
    "    training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "    xsum_filtered_train.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "    test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "    xsum_filtered_test.save_to_disk(test_input_path,fs=s3)\n",
    "else:\n",
    "    print('Please set do_save to True if you really want to save.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
